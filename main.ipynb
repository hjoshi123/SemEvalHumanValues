{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e470f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n"
     ]
    }
   ],
   "source": [
    "print('Baseline')\n",
    "\n",
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae31998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4176 entries, 3401 to 3031\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Argument ID                 4176 non-null   object\n",
      " 1   Conclusion                  4176 non-null   object\n",
      " 2   Stance                      4176 non-null   object\n",
      " 3   Premise                     4176 non-null   object\n",
      " 4   Achievement                 4176 non-null   int64 \n",
      " 5   Benevolence: caring         4176 non-null   int64 \n",
      " 6   Benevolence: dependability  4176 non-null   int64 \n",
      " 7   Conformity: interpersonal   4176 non-null   int64 \n",
      " 8   Conformity: rules           4176 non-null   int64 \n",
      " 9   Face                        4176 non-null   int64 \n",
      " 10  Hedonism                    4176 non-null   int64 \n",
      " 11  Humility                    4176 non-null   int64 \n",
      " 12  Power: dominance            4176 non-null   int64 \n",
      " 13  Power: resources            4176 non-null   int64 \n",
      " 14  Security: personal          4176 non-null   int64 \n",
      " 15  Security: societal          4176 non-null   int64 \n",
      " 16  Self-direction: action      4176 non-null   int64 \n",
      " 17  Self-direction: thought     4176 non-null   int64 \n",
      " 18  Stimulation                 4176 non-null   int64 \n",
      " 19  Tradition                   4176 non-null   int64 \n",
      " 20  Universalism: concern       4176 non-null   int64 \n",
      " 21  Universalism: nature        4176 non-null   int64 \n",
      " 22  Universalism: objectivity   4176 non-null   int64 \n",
      " 23  Universalism: tolerance     4176 non-null   int64 \n",
      "dtypes: int64(20), object(4)\n",
      "memory usage: 815.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1044 entries, 2 to 5210\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Argument ID                 1044 non-null   object\n",
      " 1   Conclusion                  1044 non-null   object\n",
      " 2   Stance                      1044 non-null   object\n",
      " 3   Premise                     1044 non-null   object\n",
      " 4   Achievement                 1044 non-null   int64 \n",
      " 5   Benevolence: caring         1044 non-null   int64 \n",
      " 6   Benevolence: dependability  1044 non-null   int64 \n",
      " 7   Conformity: interpersonal   1044 non-null   int64 \n",
      " 8   Conformity: rules           1044 non-null   int64 \n",
      " 9   Face                        1044 non-null   int64 \n",
      " 10  Hedonism                    1044 non-null   int64 \n",
      " 11  Humility                    1044 non-null   int64 \n",
      " 12  Power: dominance            1044 non-null   int64 \n",
      " 13  Power: resources            1044 non-null   int64 \n",
      " 14  Security: personal          1044 non-null   int64 \n",
      " 15  Security: societal          1044 non-null   int64 \n",
      " 16  Self-direction: action      1044 non-null   int64 \n",
      " 17  Self-direction: thought     1044 non-null   int64 \n",
      " 18  Stimulation                 1044 non-null   int64 \n",
      " 19  Tradition                   1044 non-null   int64 \n",
      " 20  Universalism: concern       1044 non-null   int64 \n",
      " 21  Universalism: nature        1044 non-null   int64 \n",
      " 22  Universalism: objectivity   1044 non-null   int64 \n",
      " 23  Universalism: tolerance     1044 non-null   int64 \n",
      "dtypes: int64(20), object(4)\n",
      "memory usage: 203.9+ KB\n"
     ]
    }
   ],
   "source": [
    "from preprocess import load_arguments, load_label, load_values_from_json, combine_columns, split_arguments\n",
    "from model import train_bert_model\n",
    "import os\n",
    "\n",
    "# Load arguments\n",
    "df_arguments = load_arguments('data/arguments-training.tsv')\n",
    "\n",
    "# Load json\n",
    "human_values = load_values_from_json('data/value-categories.json')\n",
    "\n",
    "df_labels = load_label(\"data/labels-training.tsv\", human_values[\"1\"])\n",
    "df_full_level = combine_columns(df_arguments, df_labels)\n",
    "train_arguments, valid_arguments = split_arguments(df_full_level)\n",
    "\n",
    "train_arguments.info()\n",
    "valid_arguments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d819a8c",
   "metadata": {},
   "source": [
    "## Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ab4ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Prediction\n",
      "Added predictions for 4176 arguments\n",
      "{'accuracy_thresh': 0.27624520659446716,\n",
      " 'f1-score': {'Achievement': 0.411,\n",
      "              'Benevolence: caring': 0.421,\n",
      "              'Benevolence: dependability': 0.257,\n",
      "              'Conformity: interpersonal': 0.08,\n",
      "              'Conformity: rules': 0.343,\n",
      "              'Face': 0.134,\n",
      "              'Hedonism': 0.081,\n",
      "              'Humility': 0.151,\n",
      "              'Power: dominance': 0.157,\n",
      "              'Power: resources': 0.18,\n",
      "              'Security: personal': 0.529,\n",
      "              'Security: societal': 0.448,\n",
      "              'Self-direction: action': 0.405,\n",
      "              'Self-direction: thought': 0.296,\n",
      "              'Stimulation': 0.112,\n",
      "              'Tradition': 0.194,\n",
      "              'Universalism: concern': 0.466,\n",
      "              'Universalism: nature': 0.13,\n",
      "              'Universalism: objectivity': 0.293,\n",
      "              'Universalism: tolerance': 0.241,\n",
      "              'avg-f1-score': 0.266},\n",
      " 'marco-avg-f1score': 0.266}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from baseline import random_prediction\n",
    "from metrics import compute_metrics, print_metrics\n",
    "from utils import extract_true_labels\n",
    "\n",
    "\n",
    "\n",
    "random_prediction_values = random_prediction(human_values['1'], train_arguments)\n",
    "\n",
    "metrics = compute_metrics((random_prediction_values, \n",
    "                           extract_true_labels(train_arguments, human_values['1'])), human_values['1'])\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b8590",
   "metadata": {},
   "source": [
    "## All 1 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bffc3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting all 1\n",
      "Added predictions for 4176 arguments\n",
      "{'accuracy_thresh': 0.17182710766792297,\n",
      " 'f1-score': {'Achievement': 0.427,\n",
      "              'Benevolence: caring': 0.452,\n",
      "              'Benevolence: dependability': 0.258,\n",
      "              'Conformity: interpersonal': 0.081,\n",
      "              'Conformity: rules': 0.376,\n",
      "              'Face': 0.135,\n",
      "              'Hedonism': 0.08,\n",
      "              'Humility': 0.153,\n",
      "              'Power: dominance': 0.164,\n",
      "              'Power: resources': 0.193,\n",
      "              'Security: personal': 0.547,\n",
      "              'Security: societal': 0.477,\n",
      "              'Self-direction: action': 0.407,\n",
      "              'Self-direction: thought': 0.304,\n",
      "              'Stimulation': 0.114,\n",
      "              'Tradition': 0.2,\n",
      "              'Universalism: concern': 0.55,\n",
      "              'Universalism: nature': 0.129,\n",
      "              'Universalism: objectivity': 0.301,\n",
      "              'Universalism: tolerance': 0.242,\n",
      "              'avg-f1-score': 0.28},\n",
      " 'marco-avg-f1score': 0.28}\n"
     ]
    }
   ],
   "source": [
    "from baseline import all_ones\n",
    "\n",
    "one_values = all_ones(human_values['1'], train_arguments)\n",
    "\n",
    "metrics = compute_metrics((one_values, \n",
    "                           extract_true_labels(train_arguments, human_values['1'])), human_values['1'])\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02731a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/hemantjoshi/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/hemantjoshi/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
      "loading file tokenizer.json from cache at /Users/hemantjoshi/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/hemantjoshi/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/hemantjoshi/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75244aa8a16486b94bf253c7e3013a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae193f02aaa413bbb4ad2a481bccde5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1044 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0ee14bf57b46bd9d3947a85bc01947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3c7950e952476090cedd5c468296cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file config.json from cache at /Users/hemantjoshi/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /Users/hemantjoshi/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10440\n",
      "  Number of trainable parameters = 109497620\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6728' max='10440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6728/10440 1:27:40 < 48:23, 1.28 it/s, Epoch 12.89/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Thresh</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Marco-avg-f1score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.409700</td>\n",
       "      <td>0.358689</td>\n",
       "      <td>0.856034</td>\n",
       "      <td>{'Achievement': 0.57, 'Benevolence: caring': 0.36, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.0, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.25, 'Security: personal': 0.66, 'Security: societal': 0.62, 'Self-direction: action': 0.37, 'Self-direction: thought': 0.54, 'Stimulation': 0.0, 'Tradition': 0.01, 'Universalism: concern': 0.56, 'Universalism: nature': 0.0, 'Universalism: objectivity': 0.0, 'Universalism: tolerance': 0.0, 'avg-f1-score': 0.2}</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.342900</td>\n",
       "      <td>0.330594</td>\n",
       "      <td>0.867337</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.39, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.35, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.44, 'Security: personal': 0.7, 'Security: societal': 0.68, 'Self-direction: action': 0.48, 'Self-direction: thought': 0.56, 'Stimulation': 0.0, 'Tradition': 0.36, 'Universalism: concern': 0.65, 'Universalism: nature': 0.68, 'Universalism: objectivity': 0.05, 'Universalism: tolerance': 0.14, 'avg-f1-score': 0.3}</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.298100</td>\n",
       "      <td>0.323357</td>\n",
       "      <td>0.869875</td>\n",
       "      <td>{'Achievement': 0.62, 'Benevolence: caring': 0.43, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.35, 'Face': 0.0, 'Hedonism': 0.07, 'Humility': 0.0, 'Power: dominance': 0.04, 'Power: resources': 0.41, 'Security: personal': 0.69, 'Security: societal': 0.69, 'Self-direction: action': 0.58, 'Self-direction: thought': 0.58, 'Stimulation': 0.07, 'Tradition': 0.31, 'Universalism: concern': 0.71, 'Universalism: nature': 0.78, 'Universalism: objectivity': 0.28, 'Universalism: tolerance': 0.17, 'avg-f1-score': 0.34}</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.322529</td>\n",
       "      <td>0.871791</td>\n",
       "      <td>{'Achievement': 0.62, 'Benevolence: caring': 0.44, 'Benevolence: dependability': 0.01, 'Conformity: interpersonal': 0.09, 'Conformity: rules': 0.47, 'Face': 0.0, 'Hedonism': 0.19, 'Humility': 0.0, 'Power: dominance': 0.14, 'Power: resources': 0.38, 'Security: personal': 0.66, 'Security: societal': 0.69, 'Self-direction: action': 0.61, 'Self-direction: thought': 0.62, 'Stimulation': 0.07, 'Tradition': 0.47, 'Universalism: concern': 0.7, 'Universalism: nature': 0.79, 'Universalism: objectivity': 0.27, 'Universalism: tolerance': 0.18, 'avg-f1-score': 0.37}</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.229800</td>\n",
       "      <td>0.328061</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>{'Achievement': 0.62, 'Benevolence: caring': 0.46, 'Benevolence: dependability': 0.04, 'Conformity: interpersonal': 0.05, 'Conformity: rules': 0.46, 'Face': 0.0, 'Hedonism': 0.37, 'Humility': 0.04, 'Power: dominance': 0.08, 'Power: resources': 0.37, 'Security: personal': 0.73, 'Security: societal': 0.68, 'Self-direction: action': 0.61, 'Self-direction: thought': 0.64, 'Stimulation': 0.17, 'Tradition': 0.46, 'Universalism: concern': 0.68, 'Universalism: nature': 0.77, 'Universalism: objectivity': 0.33, 'Universalism: tolerance': 0.32, 'avg-f1-score': 0.39}</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.201500</td>\n",
       "      <td>0.335690</td>\n",
       "      <td>0.868726</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.44, 'Benevolence: dependability': 0.09, 'Conformity: interpersonal': 0.09, 'Conformity: rules': 0.45, 'Face': 0.0, 'Hedonism': 0.33, 'Humility': 0.15, 'Power: dominance': 0.24, 'Power: resources': 0.44, 'Security: personal': 0.68, 'Security: societal': 0.67, 'Self-direction: action': 0.62, 'Self-direction: thought': 0.66, 'Stimulation': 0.17, 'Tradition': 0.43, 'Universalism: concern': 0.71, 'Universalism: nature': 0.74, 'Universalism: objectivity': 0.34, 'Universalism: tolerance': 0.3, 'avg-f1-score': 0.41}</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.175400</td>\n",
       "      <td>0.345241</td>\n",
       "      <td>0.864799</td>\n",
       "      <td>{'Achievement': 0.64, 'Benevolence: caring': 0.49, 'Benevolence: dependability': 0.18, 'Conformity: interpersonal': 0.14, 'Conformity: rules': 0.46, 'Face': 0.0, 'Hedonism': 0.43, 'Humility': 0.16, 'Power: dominance': 0.22, 'Power: resources': 0.49, 'Security: personal': 0.67, 'Security: societal': 0.68, 'Self-direction: action': 0.62, 'Self-direction: thought': 0.65, 'Stimulation': 0.2, 'Tradition': 0.48, 'Universalism: concern': 0.7, 'Universalism: nature': 0.76, 'Universalism: objectivity': 0.35, 'Universalism: tolerance': 0.31, 'avg-f1-score': 0.43}</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.154900</td>\n",
       "      <td>0.354180</td>\n",
       "      <td>0.866140</td>\n",
       "      <td>{'Achievement': 0.62, 'Benevolence: caring': 0.47, 'Benevolence: dependability': 0.25, 'Conformity: interpersonal': 0.18, 'Conformity: rules': 0.45, 'Face': 0.0, 'Hedonism': 0.32, 'Humility': 0.19, 'Power: dominance': 0.19, 'Power: resources': 0.43, 'Security: personal': 0.7, 'Security: societal': 0.69, 'Self-direction: action': 0.62, 'Self-direction: thought': 0.63, 'Stimulation': 0.19, 'Tradition': 0.47, 'Universalism: concern': 0.71, 'Universalism: nature': 0.77, 'Universalism: objectivity': 0.34, 'Universalism: tolerance': 0.3, 'avg-f1-score': 0.43}</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.134500</td>\n",
       "      <td>0.365614</td>\n",
       "      <td>0.865182</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.47, 'Benevolence: dependability': 0.24, 'Conformity: interpersonal': 0.14, 'Conformity: rules': 0.49, 'Face': 0.0, 'Hedonism': 0.29, 'Humility': 0.18, 'Power: dominance': 0.26, 'Power: resources': 0.43, 'Security: personal': 0.7, 'Security: societal': 0.68, 'Self-direction: action': 0.62, 'Self-direction: thought': 0.66, 'Stimulation': 0.16, 'Tradition': 0.42, 'Universalism: concern': 0.69, 'Universalism: nature': 0.75, 'Universalism: objectivity': 0.34, 'Universalism: tolerance': 0.3, 'avg-f1-score': 0.42}</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.120200</td>\n",
       "      <td>0.374920</td>\n",
       "      <td>0.863602</td>\n",
       "      <td>{'Achievement': 0.64, 'Benevolence: caring': 0.5, 'Benevolence: dependability': 0.26, 'Conformity: interpersonal': 0.14, 'Conformity: rules': 0.45, 'Face': 0.03, 'Hedonism': 0.3, 'Humility': 0.17, 'Power: dominance': 0.28, 'Power: resources': 0.45, 'Security: personal': 0.71, 'Security: societal': 0.69, 'Self-direction: action': 0.61, 'Self-direction: thought': 0.66, 'Stimulation': 0.19, 'Tradition': 0.47, 'Universalism: concern': 0.69, 'Universalism: nature': 0.76, 'Universalism: objectivity': 0.35, 'Universalism: tolerance': 0.35, 'avg-f1-score': 0.43}</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>0.384426</td>\n",
       "      <td>0.867002</td>\n",
       "      <td>{'Achievement': 0.62, 'Benevolence: caring': 0.5, 'Benevolence: dependability': 0.24, 'Conformity: interpersonal': 0.09, 'Conformity: rules': 0.45, 'Face': 0.03, 'Hedonism': 0.3, 'Humility': 0.18, 'Power: dominance': 0.24, 'Power: resources': 0.45, 'Security: personal': 0.71, 'Security: societal': 0.69, 'Self-direction: action': 0.6, 'Self-direction: thought': 0.64, 'Stimulation': 0.18, 'Tradition': 0.5, 'Universalism: concern': 0.71, 'Universalism: nature': 0.72, 'Universalism: objectivity': 0.33, 'Universalism: tolerance': 0.33, 'avg-f1-score': 0.43}</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.094300</td>\n",
       "      <td>0.393231</td>\n",
       "      <td>0.863697</td>\n",
       "      <td>{'Achievement': 0.62, 'Benevolence: caring': 0.46, 'Benevolence: dependability': 0.24, 'Conformity: interpersonal': 0.17, 'Conformity: rules': 0.43, 'Face': 0.11, 'Hedonism': 0.29, 'Humility': 0.21, 'Power: dominance': 0.24, 'Power: resources': 0.48, 'Security: personal': 0.7, 'Security: societal': 0.67, 'Self-direction: action': 0.62, 'Self-direction: thought': 0.64, 'Stimulation': 0.22, 'Tradition': 0.46, 'Universalism: concern': 0.69, 'Universalism: nature': 0.75, 'Universalism: objectivity': 0.29, 'Universalism: tolerance': 0.34, 'avg-f1-score': 0.43}</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.400301</td>\n",
       "      <td>0.864320</td>\n",
       "      <td>{'Achievement': 0.63, 'Benevolence: caring': 0.51, 'Benevolence: dependability': 0.31, 'Conformity: interpersonal': 0.2, 'Conformity: rules': 0.48, 'Face': 0.16, 'Hedonism': 0.29, 'Humility': 0.19, 'Power: dominance': 0.3, 'Power: resources': 0.44, 'Security: personal': 0.71, 'Security: societal': 0.67, 'Self-direction: action': 0.61, 'Self-direction: thought': 0.65, 'Stimulation': 0.24, 'Tradition': 0.51, 'Universalism: concern': 0.7, 'Universalism: nature': 0.73, 'Universalism: objectivity': 0.3, 'Universalism: tolerance': 0.33, 'avg-f1-score': 0.45}</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-500\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-500/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-1000\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-1000/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-1500\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-1500/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-1500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-2000\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-2000/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-2000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-2500\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-2500/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-2500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-3000\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-3000/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-3000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-3500\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-3500/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-3500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-4000\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-4000/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-4000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-4500\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-4500/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-4500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-5000\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-5000/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-5000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-5500\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-5500/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-5500/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-6000\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-6000/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-6000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/bert-base-uncased/checkpoint-6500\n",
      "Configuration saved in model/bert-base-uncased/checkpoint-6500/config.json\n",
      "Model weights saved in model/bert-base-uncased/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in model/bert-base-uncased/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in model/bert-base-uncased/checkpoint-6500/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/hemantjoshi/Documents/scratch/MS/sem1/nlp/SemEvalHumanValues/main.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hemantjoshi/Documents/scratch/MS/sem1/nlp/SemEvalHumanValues/main.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bert_model_evaluation \u001b[39m=\u001b[39m train_bert_model(train_arguments, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m'\u001b[39;49m\u001b[39mmodel/\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m'\u001b[39;49m), human_values[\u001b[39m'\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m'\u001b[39;49m], valid_arguments)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hemantjoshi/Documents/scratch/MS/sem1/nlp/SemEvalHumanValues/main.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(bert_model_evaluation[\u001b[39m'\u001b[39m\u001b[39meval_f1-score\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/scratch/MS/sem1/nlp/SemEvalHumanValues/model.py:151\u001b[0m, in \u001b[0;36mtrain_bert_model\u001b[0;34m(train_dataframe, model_dir, labels, test_dataframe, num_train_epochs)\u001b[0m\n\u001b[1;32m    140\u001b[0m model \u001b[39m=\u001b[39m load_model_from_data_dir(\u001b[39m\"\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m\"\u001b[39m, num_labels\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(labels))\n\u001b[1;32m    142\u001b[0m multi_trainer \u001b[39m=\u001b[39m MultiLabelTrainer(\n\u001b[1;32m    143\u001b[0m     model,\n\u001b[1;32m    144\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    148\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer\n\u001b[1;32m    149\u001b[0m )\n\u001b[0;32m--> 151\u001b[0m multi_trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    153\u001b[0m model\u001b[39m.\u001b[39msave_pretrained(model_dir)\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m test_dataframe \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/py39/lib/python3.9/site-packages/transformers/trainer.py:2526\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2528\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/py39/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_model_evaluation = train_bert_model(train_arguments, os.path.join('model/', 'bert-base-uncased'), human_values['1'], valid_arguments)\n",
    "print(bert_model_evaluation['eval_f1-score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "639a22baaa801c989cc954e864d538abd49c0bfeee5c51863e756a16beac61da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
