{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e470f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "print('Baseline')\n",
    "\n",
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d54d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ae31998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4176 entries, 3401 to 3031\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Argument ID                 4176 non-null   object\n",
      " 1   Conclusion                  4176 non-null   object\n",
      " 2   Stance                      4176 non-null   object\n",
      " 3   Premise                     4176 non-null   object\n",
      " 4   Achievement                 4176 non-null   int64 \n",
      " 5   Benevolence: caring         4176 non-null   int64 \n",
      " 6   Benevolence: dependability  4176 non-null   int64 \n",
      " 7   Conformity: interpersonal   4176 non-null   int64 \n",
      " 8   Conformity: rules           4176 non-null   int64 \n",
      " 9   Face                        4176 non-null   int64 \n",
      " 10  Hedonism                    4176 non-null   int64 \n",
      " 11  Humility                    4176 non-null   int64 \n",
      " 12  Power: dominance            4176 non-null   int64 \n",
      " 13  Power: resources            4176 non-null   int64 \n",
      " 14  Security: personal          4176 non-null   int64 \n",
      " 15  Security: societal          4176 non-null   int64 \n",
      " 16  Self-direction: action      4176 non-null   int64 \n",
      " 17  Self-direction: thought     4176 non-null   int64 \n",
      " 18  Stimulation                 4176 non-null   int64 \n",
      " 19  Tradition                   4176 non-null   int64 \n",
      " 20  Universalism: concern       4176 non-null   int64 \n",
      " 21  Universalism: nature        4176 non-null   int64 \n",
      " 22  Universalism: objectivity   4176 non-null   int64 \n",
      " 23  Universalism: tolerance     4176 non-null   int64 \n",
      "dtypes: int64(20), object(4)\n",
      "memory usage: 815.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1044 entries, 2 to 5210\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Argument ID                 1044 non-null   object\n",
      " 1   Conclusion                  1044 non-null   object\n",
      " 2   Stance                      1044 non-null   object\n",
      " 3   Premise                     1044 non-null   object\n",
      " 4   Achievement                 1044 non-null   int64 \n",
      " 5   Benevolence: caring         1044 non-null   int64 \n",
      " 6   Benevolence: dependability  1044 non-null   int64 \n",
      " 7   Conformity: interpersonal   1044 non-null   int64 \n",
      " 8   Conformity: rules           1044 non-null   int64 \n",
      " 9   Face                        1044 non-null   int64 \n",
      " 10  Hedonism                    1044 non-null   int64 \n",
      " 11  Humility                    1044 non-null   int64 \n",
      " 12  Power: dominance            1044 non-null   int64 \n",
      " 13  Power: resources            1044 non-null   int64 \n",
      " 14  Security: personal          1044 non-null   int64 \n",
      " 15  Security: societal          1044 non-null   int64 \n",
      " 16  Self-direction: action      1044 non-null   int64 \n",
      " 17  Self-direction: thought     1044 non-null   int64 \n",
      " 18  Stimulation                 1044 non-null   int64 \n",
      " 19  Tradition                   1044 non-null   int64 \n",
      " 20  Universalism: concern       1044 non-null   int64 \n",
      " 21  Universalism: nature        1044 non-null   int64 \n",
      " 22  Universalism: objectivity   1044 non-null   int64 \n",
      " 23  Universalism: tolerance     1044 non-null   int64 \n",
      "dtypes: int64(20), object(4)\n",
      "memory usage: 203.9+ KB\n"
     ]
    }
   ],
   "source": [
    "from preprocess import load_arguments, load_label, load_values_from_json, combine_columns, split_arguments\n",
    "from model import train_bert_model\n",
    "import os\n",
    "\n",
    "# Load arguments\n",
    "df_arguments = load_arguments('data/arguments-training.tsv')\n",
    "\n",
    "# Load json\n",
    "human_values = load_values_from_json('data/value-categories.json')\n",
    "\n",
    "df_labels = load_label(\"data/labels-training.tsv\", human_values[\"1\"])\n",
    "df_full_level = combine_columns(df_arguments, df_labels)\n",
    "# train_arguments, valid_arguments = split_arguments(df_full_level)\n",
    "\n",
    "train_arguments = df_full_level\n",
    "\n",
    "df_valid_arguments = load_arguments('data/arguments-validation.tsv')\n",
    "df_valid_labels = load_label(\"data/labels-validation.tsv\", human_values[\"1\"])\n",
    "df_valid_full_level = combine_columns(df_valid_arguments, df_valid_labels)\n",
    "\n",
    "valid_arguments = df_valid_full_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d819a8c",
   "metadata": {},
   "source": [
    "## Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2ab4ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Prediction\n",
      "Added predictions for 4176 arguments\n",
      "{'accuracy_thresh': 0.27721503376960754,\n",
      " 'f1-score': {'Achievement': 0.403,\n",
      "              'Benevolence: caring': 0.401,\n",
      "              'Benevolence: dependability': 0.254,\n",
      "              'Conformity: interpersonal': 0.082,\n",
      "              'Conformity: rules': 0.356,\n",
      "              'Face': 0.135,\n",
      "              'Hedonism': 0.078,\n",
      "              'Humility': 0.157,\n",
      "              'Power: dominance': 0.16,\n",
      "              'Power: resources': 0.182,\n",
      "              'Security: personal': 0.524,\n",
      "              'Security: societal': 0.447,\n",
      "              'Self-direction: action': 0.408,\n",
      "              'Self-direction: thought': 0.297,\n",
      "              'Stimulation': 0.107,\n",
      "              'Tradition': 0.197,\n",
      "              'Universalism: concern': 0.476,\n",
      "              'Universalism: nature': 0.124,\n",
      "              'Universalism: objectivity': 0.29,\n",
      "              'Universalism: tolerance': 0.227,\n",
      "              'avg-f1-score': 0.265},\n",
      " 'marco-avg-f1score': 0.265}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from baseline import random_prediction\n",
    "from metrics import compute_metrics, print_metrics\n",
    "from utils import extract_true_labels\n",
    "\n",
    "\n",
    "\n",
    "random_prediction_values = random_prediction(human_values['1'], train_arguments)\n",
    "\n",
    "metrics = compute_metrics((random_prediction_values, \n",
    "                           extract_true_labels(train_arguments, human_values['1'])), human_values['1'])\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b8590",
   "metadata": {},
   "source": [
    "## All 1 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bffc3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting all 1\n",
      "Added predictions for 4176 arguments\n",
      "{'accuracy_thresh': 0.17182710766792297,\n",
      " 'f1-score': {'Achievement': 0.427,\n",
      "              'Benevolence: caring': 0.452,\n",
      "              'Benevolence: dependability': 0.258,\n",
      "              'Conformity: interpersonal': 0.081,\n",
      "              'Conformity: rules': 0.376,\n",
      "              'Face': 0.135,\n",
      "              'Hedonism': 0.08,\n",
      "              'Humility': 0.153,\n",
      "              'Power: dominance': 0.164,\n",
      "              'Power: resources': 0.193,\n",
      "              'Security: personal': 0.547,\n",
      "              'Security: societal': 0.477,\n",
      "              'Self-direction: action': 0.407,\n",
      "              'Self-direction: thought': 0.304,\n",
      "              'Stimulation': 0.114,\n",
      "              'Tradition': 0.2,\n",
      "              'Universalism: concern': 0.55,\n",
      "              'Universalism: nature': 0.129,\n",
      "              'Universalism: objectivity': 0.301,\n",
      "              'Universalism: tolerance': 0.242,\n",
      "              'avg-f1-score': 0.28},\n",
      " 'marco-avg-f1score': 0.28}\n"
     ]
    }
   ],
   "source": [
    "from baseline import all_ones\n",
    "\n",
    "one_values = all_ones(human_values['1'], train_arguments)\n",
    "\n",
    "metrics = compute_metrics((one_values, \n",
    "                           extract_true_labels(train_arguments, human_values['1'])), human_values['1'])\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02731a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec43cd6be15d4244b5f4d9e367bf6840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abd04532cc04df8bff9f0da0c34d5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1044 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f77034335e8c48518b13c113b6f9f951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae0a80e13764480ba94152eeac871d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using `logging_steps` to initialize `eval_steps` to 500\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\akshi\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10440\n",
      "  Number of trainable parameters = 28773908\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9268' max='10440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9268/10440 2:10:43 < 16:32, 1.18 it/s, Epoch 17.75/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Thresh</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Marco-avg-f1score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.430600</td>\n",
       "      <td>0.380950</td>\n",
       "      <td>0.846169</td>\n",
       "      <td>{'Achievement': 0.27, 'Benevolence: caring': 0.05, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.0, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.0, 'Security: personal': 0.59, 'Security: societal': 0.5, 'Self-direction: action': 0.02, 'Self-direction: thought': 0.0, 'Stimulation': 0.0, 'Tradition': 0.0, 'Universalism: concern': 0.52, 'Universalism: nature': 0.0, 'Universalism: objectivity': 0.0, 'Universalism: tolerance': 0.0, 'avg-f1-score': 0.1}</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.354642</td>\n",
       "      <td>0.857711</td>\n",
       "      <td>{'Achievement': 0.58, 'Benevolence: caring': 0.28, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.05, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.29, 'Security: personal': 0.68, 'Security: societal': 0.56, 'Self-direction: action': 0.35, 'Self-direction: thought': 0.46, 'Stimulation': 0.0, 'Tradition': 0.04, 'Universalism: concern': 0.51, 'Universalism: nature': 0.27, 'Universalism: objectivity': 0.0, 'Universalism: tolerance': 0.0, 'avg-f1-score': 0.2}</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.341015</td>\n",
       "      <td>0.863171</td>\n",
       "      <td>{'Achievement': 0.56, 'Benevolence: caring': 0.37, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.22, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.32, 'Security: personal': 0.68, 'Security: societal': 0.67, 'Self-direction: action': 0.46, 'Self-direction: thought': 0.55, 'Stimulation': 0.0, 'Tradition': 0.15, 'Universalism: concern': 0.66, 'Universalism: nature': 0.7, 'Universalism: objectivity': 0.1, 'Universalism: tolerance': 0.03, 'avg-f1-score': 0.27}</td>\n",
       "      <td>0.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.323300</td>\n",
       "      <td>0.334286</td>\n",
       "      <td>0.866188</td>\n",
       "      <td>{'Achievement': 0.58, 'Benevolence: caring': 0.42, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.27, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.44, 'Security: personal': 0.7, 'Security: societal': 0.63, 'Self-direction: action': 0.59, 'Self-direction: thought': 0.57, 'Stimulation': 0.0, 'Tradition': 0.3, 'Universalism: concern': 0.62, 'Universalism: nature': 0.72, 'Universalism: objectivity': 0.17, 'Universalism: tolerance': 0.15, 'avg-f1-score': 0.31}</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.330607</td>\n",
       "      <td>0.866715</td>\n",
       "      <td>{'Achievement': 0.6, 'Benevolence: caring': 0.44, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.35, 'Face': 0.0, 'Hedonism': 0.13, 'Humility': 0.0, 'Power: dominance': 0.04, 'Power: resources': 0.49, 'Security: personal': 0.69, 'Security: societal': 0.65, 'Self-direction: action': 0.54, 'Self-direction: thought': 0.59, 'Stimulation': 0.03, 'Tradition': 0.34, 'Universalism: concern': 0.69, 'Universalism: nature': 0.74, 'Universalism: objectivity': 0.21, 'Universalism: tolerance': 0.19, 'avg-f1-score': 0.34}</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.333998</td>\n",
       "      <td>0.864847</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.45, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.38, 'Face': 0.0, 'Hedonism': 0.18, 'Humility': 0.0, 'Power: dominance': 0.06, 'Power: resources': 0.51, 'Security: personal': 0.67, 'Security: societal': 0.65, 'Self-direction: action': 0.57, 'Self-direction: thought': 0.61, 'Stimulation': 0.1, 'Tradition': 0.41, 'Universalism: concern': 0.69, 'Universalism: nature': 0.77, 'Universalism: objectivity': 0.28, 'Universalism: tolerance': 0.21, 'avg-f1-score': 0.36}</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.272500</td>\n",
       "      <td>0.328541</td>\n",
       "      <td>0.867433</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.44, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.39, 'Face': 0.0, 'Hedonism': 0.13, 'Humility': 0.0, 'Power: dominance': 0.04, 'Power: resources': 0.49, 'Security: personal': 0.68, 'Security: societal': 0.66, 'Self-direction: action': 0.53, 'Self-direction: thought': 0.6, 'Stimulation': 0.03, 'Tradition': 0.41, 'Universalism: concern': 0.7, 'Universalism: nature': 0.77, 'Universalism: objectivity': 0.23, 'Universalism: tolerance': 0.18, 'avg-f1-score': 0.34}</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.261700</td>\n",
       "      <td>0.329824</td>\n",
       "      <td>0.866188</td>\n",
       "      <td>{'Achievement': 0.6, 'Benevolence: caring': 0.37, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.37, 'Face': 0.0, 'Hedonism': 0.13, 'Humility': 0.0, 'Power: dominance': 0.06, 'Power: resources': 0.49, 'Security: personal': 0.7, 'Security: societal': 0.67, 'Self-direction: action': 0.51, 'Self-direction: thought': 0.55, 'Stimulation': 0.03, 'Tradition': 0.38, 'Universalism: concern': 0.69, 'Universalism: nature': 0.75, 'Universalism: objectivity': 0.28, 'Universalism: tolerance': 0.26, 'avg-f1-score': 0.34}</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.246700</td>\n",
       "      <td>0.333564</td>\n",
       "      <td>0.865517</td>\n",
       "      <td>{'Achievement': 0.6, 'Benevolence: caring': 0.47, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.44, 'Face': 0.0, 'Hedonism': 0.12, 'Humility': 0.0, 'Power: dominance': 0.08, 'Power: resources': 0.47, 'Security: personal': 0.69, 'Security: societal': 0.68, 'Self-direction: action': 0.53, 'Self-direction: thought': 0.59, 'Stimulation': 0.07, 'Tradition': 0.44, 'Universalism: concern': 0.71, 'Universalism: nature': 0.75, 'Universalism: objectivity': 0.22, 'Universalism: tolerance': 0.25, 'avg-f1-score': 0.36}</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.237800</td>\n",
       "      <td>0.334196</td>\n",
       "      <td>0.865661</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.47, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.39, 'Face': 0.0, 'Hedonism': 0.18, 'Humility': 0.02, 'Power: dominance': 0.1, 'Power: resources': 0.47, 'Security: personal': 0.68, 'Security: societal': 0.67, 'Self-direction: action': 0.56, 'Self-direction: thought': 0.62, 'Stimulation': 0.12, 'Tradition': 0.38, 'Universalism: concern': 0.71, 'Universalism: nature': 0.77, 'Universalism: objectivity': 0.29, 'Universalism: tolerance': 0.27, 'avg-f1-score': 0.37}</td>\n",
       "      <td>0.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.226600</td>\n",
       "      <td>0.336163</td>\n",
       "      <td>0.864416</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.44, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.09, 'Conformity: rules': 0.38, 'Face': 0.03, 'Hedonism': 0.23, 'Humility': 0.02, 'Power: dominance': 0.1, 'Power: resources': 0.44, 'Security: personal': 0.68, 'Security: societal': 0.67, 'Self-direction: action': 0.55, 'Self-direction: thought': 0.61, 'Stimulation': 0.12, 'Tradition': 0.44, 'Universalism: concern': 0.7, 'Universalism: nature': 0.76, 'Universalism: objectivity': 0.33, 'Universalism: tolerance': 0.3, 'avg-f1-score': 0.38}</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>0.340302</td>\n",
       "      <td>0.865326</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.47, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.39, 'Face': 0.03, 'Hedonism': 0.18, 'Humility': 0.02, 'Power: dominance': 0.08, 'Power: resources': 0.48, 'Security: personal': 0.69, 'Security: societal': 0.68, 'Self-direction: action': 0.55, 'Self-direction: thought': 0.61, 'Stimulation': 0.09, 'Tradition': 0.37, 'Universalism: concern': 0.69, 'Universalism: nature': 0.76, 'Universalism: objectivity': 0.31, 'Universalism: tolerance': 0.28, 'avg-f1-score': 0.36}</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.211700</td>\n",
       "      <td>0.343114</td>\n",
       "      <td>0.861207</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.46, 'Benevolence: dependability': 0.03, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.41, 'Face': 0.08, 'Hedonism': 0.22, 'Humility': 0.04, 'Power: dominance': 0.13, 'Power: resources': 0.48, 'Security: personal': 0.68, 'Security: societal': 0.66, 'Self-direction: action': 0.53, 'Self-direction: thought': 0.61, 'Stimulation': 0.2, 'Tradition': 0.43, 'Universalism: concern': 0.7, 'Universalism: nature': 0.78, 'Universalism: objectivity': 0.31, 'Universalism: tolerance': 0.3, 'avg-f1-score': 0.38}</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.202400</td>\n",
       "      <td>0.344509</td>\n",
       "      <td>0.862356</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.46, 'Benevolence: dependability': 0.04, 'Conformity: interpersonal': 0.04, 'Conformity: rules': 0.44, 'Face': 0.05, 'Hedonism': 0.22, 'Humility': 0.04, 'Power: dominance': 0.12, 'Power: resources': 0.46, 'Security: personal': 0.68, 'Security: societal': 0.65, 'Self-direction: action': 0.54, 'Self-direction: thought': 0.6, 'Stimulation': 0.2, 'Tradition': 0.45, 'Universalism: concern': 0.69, 'Universalism: nature': 0.77, 'Universalism: objectivity': 0.3, 'Universalism: tolerance': 0.31, 'avg-f1-score': 0.38}</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.196200</td>\n",
       "      <td>0.345777</td>\n",
       "      <td>0.864559</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.46, 'Benevolence: dependability': 0.01, 'Conformity: interpersonal': 0.04, 'Conformity: rules': 0.42, 'Face': 0.08, 'Hedonism': 0.18, 'Humility': 0.04, 'Power: dominance': 0.1, 'Power: resources': 0.48, 'Security: personal': 0.68, 'Security: societal': 0.66, 'Self-direction: action': 0.53, 'Self-direction: thought': 0.58, 'Stimulation': 0.23, 'Tradition': 0.42, 'Universalism: concern': 0.7, 'Universalism: nature': 0.77, 'Universalism: objectivity': 0.31, 'Universalism: tolerance': 0.31, 'avg-f1-score': 0.38}</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.347192</td>\n",
       "      <td>0.862117</td>\n",
       "      <td>{'Achievement': 0.62, 'Benevolence: caring': 0.46, 'Benevolence: dependability': 0.07, 'Conformity: interpersonal': 0.04, 'Conformity: rules': 0.44, 'Face': 0.08, 'Hedonism': 0.18, 'Humility': 0.02, 'Power: dominance': 0.13, 'Power: resources': 0.46, 'Security: personal': 0.67, 'Security: societal': 0.66, 'Self-direction: action': 0.55, 'Self-direction: thought': 0.61, 'Stimulation': 0.2, 'Tradition': 0.4, 'Universalism: concern': 0.7, 'Universalism: nature': 0.76, 'Universalism: objectivity': 0.3, 'Universalism: tolerance': 0.28, 'avg-f1-score': 0.38}</td>\n",
       "      <td>0.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.186100</td>\n",
       "      <td>0.350327</td>\n",
       "      <td>0.861398</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.45, 'Benevolence: dependability': 0.07, 'Conformity: interpersonal': 0.08, 'Conformity: rules': 0.44, 'Face': 0.08, 'Hedonism': 0.22, 'Humility': 0.04, 'Power: dominance': 0.13, 'Power: resources': 0.46, 'Security: personal': 0.67, 'Security: societal': 0.66, 'Self-direction: action': 0.55, 'Self-direction: thought': 0.6, 'Stimulation': 0.22, 'Tradition': 0.45, 'Universalism: concern': 0.68, 'Universalism: nature': 0.76, 'Universalism: objectivity': 0.28, 'Universalism: tolerance': 0.28, 'avg-f1-score': 0.39}</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.184600</td>\n",
       "      <td>0.351487</td>\n",
       "      <td>0.860153</td>\n",
       "      <td>{'Achievement': 0.61, 'Benevolence: caring': 0.45, 'Benevolence: dependability': 0.11, 'Conformity: interpersonal': 0.08, 'Conformity: rules': 0.44, 'Face': 0.08, 'Hedonism': 0.22, 'Humility': 0.08, 'Power: dominance': 0.11, 'Power: resources': 0.45, 'Security: personal': 0.67, 'Security: societal': 0.66, 'Self-direction: action': 0.55, 'Self-direction: thought': 0.59, 'Stimulation': 0.22, 'Tradition': 0.42, 'Universalism: concern': 0.69, 'Universalism: nature': 0.77, 'Universalism: objectivity': 0.32, 'Universalism: tolerance': 0.32, 'avg-f1-score': 0.39}</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-500\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-500\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-1000\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-1000\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-1000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-1500\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-1500\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-1500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-2000\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-2000\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-2000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-2500\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-2500\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-2500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-3000\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-3000\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-3000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-3500\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-3500\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-3500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-4000\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-4000\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-4000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-4500\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-4500\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-4500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-5000\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-5000\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-5000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-5500\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-5500\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-5500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-6000\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-6000\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-6000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-6500\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-6500\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-6500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-7000\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-7000\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-7000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-7500\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-7500\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-7500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-8000\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-8000\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-8000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-8500\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-8500\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-8500\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-8500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1044\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to model/prajjwal1/bert-small\\checkpoint-9000\n",
      "Configuration saved in model/prajjwal1/bert-small\\checkpoint-9000\\config.json\n",
      "Model weights saved in model/prajjwal1/bert-small\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in model/prajjwal1/bert-small\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in model/prajjwal1/bert-small\\checkpoint-9000\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "bert_model_evaluation = train_bert_model(train_arguments, os.path.join('model/', 'prajjwal1/bert-small'), human_values['1'], valid_arguments)\n",
    "print(bert_model_evaluation['eval_f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49a51594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3503268361091614, 'eval_accuracy_thresh': 0.861398458480835, 'eval_f1-score': {'Achievement': 0.61, 'Benevolence: caring': 0.45, 'Benevolence: dependability': 0.07, 'Conformity: interpersonal': 0.08, 'Conformity: rules': 0.44, 'Face': 0.08, 'Hedonism': 0.22, 'Humility': 0.04, 'Power: dominance': 0.13, 'Power: resources': 0.46, 'Security: personal': 0.67, 'Security: societal': 0.66, 'Self-direction: action': 0.55, 'Self-direction: thought': 0.6, 'Stimulation': 0.22, 'Tradition': 0.45, 'Universalism: concern': 0.68, 'Universalism: nature': 0.76, 'Universalism: objectivity': 0.28, 'Universalism: tolerance': 0.28, 'avg-f1-score': 0.39}, 'eval_marco-avg-f1score': 0.39, 'eval_runtime': 18.0899, 'eval_samples_per_second': 57.712, 'eval_steps_per_second': 7.242, 'epoch': 20.0}\n"
     ]
    }
   ],
   "source": [
    "print(bert_model_evaluation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "639a22baaa801c989cc954e864d538abd49c0bfeee5c51863e756a16beac61da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
