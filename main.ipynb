{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e470f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n"
     ]
    }
   ],
   "source": [
    "print('Baseline')\n",
    "\n",
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d54d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae31998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'AttributeError'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4176 entries, 3401 to 3031\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Argument ID                 4176 non-null   object\n",
      " 1   Conclusion                  4176 non-null   object\n",
      " 2   Stance                      4176 non-null   object\n",
      " 3   Premise                     4176 non-null   object\n",
      " 4   Achievement                 4176 non-null   int64 \n",
      " 5   Benevolence: caring         4176 non-null   int64 \n",
      " 6   Benevolence: dependability  4176 non-null   int64 \n",
      " 7   Conformity: interpersonal   4176 non-null   int64 \n",
      " 8   Conformity: rules           4176 non-null   int64 \n",
      " 9   Face                        4176 non-null   int64 \n",
      " 10  Hedonism                    4176 non-null   int64 \n",
      " 11  Humility                    4176 non-null   int64 \n",
      " 12  Power: dominance            4176 non-null   int64 \n",
      " 13  Power: resources            4176 non-null   int64 \n",
      " 14  Security: personal          4176 non-null   int64 \n",
      " 15  Security: societal          4176 non-null   int64 \n",
      " 16  Self-direction: action      4176 non-null   int64 \n",
      " 17  Self-direction: thought     4176 non-null   int64 \n",
      " 18  Stimulation                 4176 non-null   int64 \n",
      " 19  Tradition                   4176 non-null   int64 \n",
      " 20  Universalism: concern       4176 non-null   int64 \n",
      " 21  Universalism: nature        4176 non-null   int64 \n",
      " 22  Universalism: objectivity   4176 non-null   int64 \n",
      " 23  Universalism: tolerance     4176 non-null   int64 \n",
      "dtypes: int64(20), object(4)\n",
      "memory usage: 815.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1044 entries, 2 to 5210\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Argument ID                 1044 non-null   object\n",
      " 1   Conclusion                  1044 non-null   object\n",
      " 2   Stance                      1044 non-null   object\n",
      " 3   Premise                     1044 non-null   object\n",
      " 4   Achievement                 1044 non-null   int64 \n",
      " 5   Benevolence: caring         1044 non-null   int64 \n",
      " 6   Benevolence: dependability  1044 non-null   int64 \n",
      " 7   Conformity: interpersonal   1044 non-null   int64 \n",
      " 8   Conformity: rules           1044 non-null   int64 \n",
      " 9   Face                        1044 non-null   int64 \n",
      " 10  Hedonism                    1044 non-null   int64 \n",
      " 11  Humility                    1044 non-null   int64 \n",
      " 12  Power: dominance            1044 non-null   int64 \n",
      " 13  Power: resources            1044 non-null   int64 \n",
      " 14  Security: personal          1044 non-null   int64 \n",
      " 15  Security: societal          1044 non-null   int64 \n",
      " 16  Self-direction: action      1044 non-null   int64 \n",
      " 17  Self-direction: thought     1044 non-null   int64 \n",
      " 18  Stimulation                 1044 non-null   int64 \n",
      " 19  Tradition                   1044 non-null   int64 \n",
      " 20  Universalism: concern       1044 non-null   int64 \n",
      " 21  Universalism: nature        1044 non-null   int64 \n",
      " 22  Universalism: objectivity   1044 non-null   int64 \n",
      " 23  Universalism: tolerance     1044 non-null   int64 \n",
      "dtypes: int64(20), object(4)\n",
      "memory usage: 203.9+ KB\n"
     ]
    }
   ],
   "source": [
    "from preprocess import load_arguments, load_label, load_values_from_json, combine_columns, split_arguments\n",
    "from model import train_bert_model\n",
    "import os\n",
    "\n",
    "# Load arguments\n",
    "df_arguments = load_arguments('data/arguments-training.tsv')\n",
    "\n",
    "# Load json\n",
    "human_values = load_values_from_json('data/value-categories.json')\n",
    "\n",
    "df_labels = load_label(\"data/labels-training.tsv\", human_values[\"1\"])\n",
    "df_full_level = combine_columns(df_arguments, df_labels)\n",
    "train_arguments, valid_arguments = split_arguments(df_full_level)\n",
    "\n",
    "train_arguments.info()\n",
    "valid_arguments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d819a8c",
   "metadata": {},
   "source": [
    "## Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ab4ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Prediction\n",
      "Added predictions for 4176 arguments\n",
      "{'accuracy_thresh': 0.27721503376960754,\n",
      " 'f1-score': {'Achievement': 0.403,\n",
      "              'Benevolence: caring': 0.401,\n",
      "              'Benevolence: dependability': 0.254,\n",
      "              'Conformity: interpersonal': 0.082,\n",
      "              'Conformity: rules': 0.356,\n",
      "              'Face': 0.135,\n",
      "              'Hedonism': 0.078,\n",
      "              'Humility': 0.157,\n",
      "              'Power: dominance': 0.16,\n",
      "              'Power: resources': 0.182,\n",
      "              'Security: personal': 0.524,\n",
      "              'Security: societal': 0.447,\n",
      "              'Self-direction: action': 0.408,\n",
      "              'Self-direction: thought': 0.297,\n",
      "              'Stimulation': 0.107,\n",
      "              'Tradition': 0.197,\n",
      "              'Universalism: concern': 0.476,\n",
      "              'Universalism: nature': 0.124,\n",
      "              'Universalism: objectivity': 0.29,\n",
      "              'Universalism: tolerance': 0.227,\n",
      "              'avg-f1-score': 0.265},\n",
      " 'marco-avg-f1score': 0.265}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from baseline import random_prediction\n",
    "from metrics import compute_metrics, print_metrics\n",
    "from utils import extract_true_labels\n",
    "\n",
    "\n",
    "\n",
    "random_prediction_values = random_prediction(human_values['1'], train_arguments)\n",
    "\n",
    "metrics = compute_metrics((random_prediction_values, \n",
    "                           extract_true_labels(train_arguments, human_values['1'])), human_values['1'])\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b8590",
   "metadata": {},
   "source": [
    "## All 1 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline import all_ones\n",
    "\n",
    "one_values = all_ones(human_values['1'], train_arguments)\n",
    "\n",
    "metrics = compute_metrics((one_values, \n",
    "                           extract_true_labels(train_arguments, human_values['1'])), human_values['1'])\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02731a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0ae290bcef4235bc825f21942402b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6549c6765a364e2fbc4b3d5da72bf88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b39d99ee689849af8b4f2c8c4887628c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108ad8205e414168858220fb3887e130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\akshi\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10440\n",
      "  Number of trainable parameters = 28773908\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9001' max='10440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 9001/10440 2:37:45 < 25:13, 0.95 it/s, Epoch 17.24/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy Thresh</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Marco-avg-f1score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Avg-precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Avg-recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.428500</td>\n",
       "      <td>0.382404</td>\n",
       "      <td>0.845558</td>\n",
       "      <td>{'Achievement': 0.13, 'Benevolence: caring': 0.01, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.0, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.0, 'Security: personal': 0.62, 'Security: societal': 0.56, 'Self-direction: action': 0.0, 'Self-direction: thought': 0.0, 'Stimulation': 0.0, 'Tradition': 0.0, 'Universalism: concern': 0.6, 'Universalism: nature': 0.0, 'Universalism: objectivity': 0.0, 'Universalism: tolerance': 0.0, 'avg-f1-score': 0.1}</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>{'Achievement': 0.81, 'Benevolence: caring': 1.0, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.0, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.0, 'Security: personal': 0.67, 'Security: societal': 0.76, 'Self-direction: action': 0.0, 'Self-direction: thought': 0.0, 'Stimulation': 0.0, 'Tradition': 0.0, 'Universalism: concern': 0.76, 'Universalism: nature': 0.0, 'Universalism: objectivity': 0.0, 'Universalism: tolerance': 0.0, 'avg-precision': 0.2}</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>{'Achievement': 0.07, 'Benevolence: caring': 0.0, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.0, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.0, 'Security: personal': 0.58, 'Security: societal': 0.44, 'Self-direction: action': 0.0, 'Self-direction: thought': 0.0, 'Stimulation': 0.0, 'Tradition': 0.0, 'Universalism: concern': 0.49, 'Universalism: nature': 0.0, 'Universalism: objectivity': 0.0, 'Universalism: tolerance': 0.0, 'avg-recall': 0.08}</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.370300</td>\n",
       "      <td>0.341273</td>\n",
       "      <td>0.864296</td>\n",
       "      <td>{'Achievement': 0.62, 'Benevolence: caring': 0.26, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.08, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.45, 'Security: personal': 0.72, 'Security: societal': 0.64, 'Self-direction: action': 0.39, 'Self-direction: thought': 0.44, 'Stimulation': 0.0, 'Tradition': 0.18, 'Universalism: concern': 0.64, 'Universalism: nature': 0.37, 'Universalism: objectivity': 0.08, 'Universalism: tolerance': 0.0, 'avg-f1-score': 0.24}</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>{'Achievement': 0.74, 'Benevolence: caring': 0.84, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.75, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.8, 'Security: personal': 0.74, 'Security: societal': 0.79, 'Self-direction: action': 0.8, 'Self-direction: thought': 0.8, 'Stimulation': 0.0, 'Tradition': 0.9, 'Universalism: concern': 0.77, 'Universalism: nature': 0.96, 'Universalism: objectivity': 0.8, 'Universalism: tolerance': 0.0, 'avg-precision': 0.48}</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>{'Achievement': 0.54, 'Benevolence: caring': 0.15, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.04, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.31, 'Security: personal': 0.7, 'Security: societal': 0.53, 'Self-direction: action': 0.26, 'Self-direction: thought': 0.3, 'Stimulation': 0.0, 'Tradition': 0.1, 'Universalism: concern': 0.55, 'Universalism: nature': 0.23, 'Universalism: objectivity': 0.04, 'Universalism: tolerance': 0.0, 'avg-recall': 0.19}</td>\n",
       "      <td>0.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.338900</td>\n",
       "      <td>0.316818</td>\n",
       "      <td>0.876916</td>\n",
       "      <td>{'Achievement': 0.65, 'Benevolence: caring': 0.34, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.23, 'Face': 0.01, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.49, 'Security: personal': 0.75, 'Security: societal': 0.75, 'Self-direction: action': 0.6, 'Self-direction: thought': 0.63, 'Stimulation': 0.0, 'Tradition': 0.33, 'Universalism: concern': 0.72, 'Universalism: nature': 0.61, 'Universalism: objectivity': 0.18, 'Universalism: tolerance': 0.16, 'avg-f1-score': 0.32}</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>{'Achievement': 0.81, 'Benevolence: caring': 0.86, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.73, 'Face': 1.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.83, 'Security: personal': 0.78, 'Security: societal': 0.77, 'Self-direction: action': 0.82, 'Self-direction: thought': 0.79, 'Stimulation': 0.0, 'Tradition': 0.79, 'Universalism: concern': 0.75, 'Universalism: nature': 0.94, 'Universalism: objectivity': 0.78, 'Universalism: tolerance': 0.74, 'avg-precision': 0.57}</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>{'Achievement': 0.55, 'Benevolence: caring': 0.22, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.14, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.0, 'Power: dominance': 0.0, 'Power: resources': 0.35, 'Security: personal': 0.72, 'Security: societal': 0.73, 'Self-direction: action': 0.47, 'Self-direction: thought': 0.52, 'Stimulation': 0.0, 'Tradition': 0.21, 'Universalism: concern': 0.69, 'Universalism: nature': 0.45, 'Universalism: objectivity': 0.1, 'Universalism: tolerance': 0.09, 'avg-recall': 0.26}</td>\n",
       "      <td>0.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.321000</td>\n",
       "      <td>0.295352</td>\n",
       "      <td>0.884950</td>\n",
       "      <td>{'Achievement': 0.66, 'Benevolence: caring': 0.46, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.38, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.02, 'Power: dominance': 0.0, 'Power: resources': 0.68, 'Security: personal': 0.77, 'Security: societal': 0.75, 'Self-direction: action': 0.7, 'Self-direction: thought': 0.67, 'Stimulation': 0.02, 'Tradition': 0.52, 'Universalism: concern': 0.72, 'Universalism: nature': 0.65, 'Universalism: objectivity': 0.25, 'Universalism: tolerance': 0.2, 'avg-f1-score': 0.37}</td>\n",
       "      <td>0.370000</td>\n",
       "      <td>{'Achievement': 0.87, 'Benevolence: caring': 0.8, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.75, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 1.0, 'Power: dominance': 0.0, 'Power: resources': 0.79, 'Security: personal': 0.81, 'Security: societal': 0.81, 'Self-direction: action': 0.78, 'Self-direction: thought': 0.83, 'Stimulation': 1.0, 'Tradition': 0.79, 'Universalism: concern': 0.82, 'Universalism: nature': 0.95, 'Universalism: objectivity': 0.74, 'Universalism: tolerance': 0.79, 'avg-precision': 0.63}</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>{'Achievement': 0.53, 'Benevolence: caring': 0.32, 'Benevolence: dependability': 0.0, 'Conformity: interpersonal': 0.0, 'Conformity: rules': 0.26, 'Face': 0.0, 'Hedonism': 0.0, 'Humility': 0.01, 'Power: dominance': 0.0, 'Power: resources': 0.59, 'Security: personal': 0.74, 'Security: societal': 0.7, 'Self-direction: action': 0.63, 'Self-direction: thought': 0.56, 'Stimulation': 0.01, 'Tradition': 0.38, 'Universalism: concern': 0.64, 'Universalism: nature': 0.49, 'Universalism: objectivity': 0.15, 'Universalism: tolerance': 0.11, 'avg-recall': 0.31}</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.301000</td>\n",
       "      <td>0.276332</td>\n",
       "      <td>0.893906</td>\n",
       "      <td>{'Achievement': 0.73, 'Benevolence: caring': 0.53, 'Benevolence: dependability': 0.02, 'Conformity: interpersonal': 0.01, 'Conformity: rules': 0.54, 'Face': 0.06, 'Hedonism': 0.12, 'Humility': 0.13, 'Power: dominance': 0.0, 'Power: resources': 0.74, 'Security: personal': 0.8, 'Security: societal': 0.78, 'Self-direction: action': 0.71, 'Self-direction: thought': 0.73, 'Stimulation': 0.1, 'Tradition': 0.53, 'Universalism: concern': 0.74, 'Universalism: nature': 0.74, 'Universalism: objectivity': 0.32, 'Universalism: tolerance': 0.28, 'avg-f1-score': 0.43}</td>\n",
       "      <td>0.430000</td>\n",
       "      <td>{'Achievement': 0.84, 'Benevolence: caring': 0.79, 'Benevolence: dependability': 0.86, 'Conformity: interpersonal': 1.0, 'Conformity: rules': 0.73, 'Face': 0.75, 'Hedonism': 0.79, 'Humility': 0.93, 'Power: dominance': 0.0, 'Power: resources': 0.83, 'Security: personal': 0.82, 'Security: societal': 0.86, 'Self-direction: action': 0.85, 'Self-direction: thought': 0.82, 'Stimulation': 0.82, 'Tradition': 0.87, 'Universalism: concern': 0.84, 'Universalism: nature': 0.93, 'Universalism: objectivity': 0.75, 'Universalism: tolerance': 0.79, 'avg-precision': 0.79}</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>{'Achievement': 0.66, 'Benevolence: caring': 0.4, 'Benevolence: dependability': 0.01, 'Conformity: interpersonal': 0.01, 'Conformity: rules': 0.43, 'Face': 0.03, 'Hedonism': 0.06, 'Humility': 0.07, 'Power: dominance': 0.0, 'Power: resources': 0.66, 'Security: personal': 0.78, 'Security: societal': 0.7, 'Self-direction: action': 0.61, 'Self-direction: thought': 0.66, 'Stimulation': 0.06, 'Tradition': 0.38, 'Universalism: concern': 0.66, 'Universalism: nature': 0.61, 'Universalism: objectivity': 0.2, 'Universalism: tolerance': 0.17, 'avg-recall': 0.36}</td>\n",
       "      <td>0.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.287100</td>\n",
       "      <td>0.261604</td>\n",
       "      <td>0.902634</td>\n",
       "      <td>{'Achievement': 0.76, 'Benevolence: caring': 0.64, 'Benevolence: dependability': 0.05, 'Conformity: interpersonal': 0.08, 'Conformity: rules': 0.6, 'Face': 0.09, 'Hedonism': 0.36, 'Humility': 0.16, 'Power: dominance': 0.08, 'Power: resources': 0.76, 'Security: personal': 0.82, 'Security: societal': 0.8, 'Self-direction: action': 0.77, 'Self-direction: thought': 0.8, 'Stimulation': 0.16, 'Tradition': 0.63, 'Universalism: concern': 0.77, 'Universalism: nature': 0.79, 'Universalism: objectivity': 0.42, 'Universalism: tolerance': 0.31, 'avg-f1-score': 0.49}</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>{'Achievement': 0.84, 'Benevolence: caring': 0.74, 'Benevolence: dependability': 0.84, 'Conformity: interpersonal': 0.88, 'Conformity: rules': 0.76, 'Face': 0.82, 'Hedonism': 0.95, 'Humility': 0.97, 'Power: dominance': 1.0, 'Power: resources': 0.86, 'Security: personal': 0.86, 'Security: societal': 0.9, 'Self-direction: action': 0.84, 'Self-direction: thought': 0.77, 'Stimulation': 0.88, 'Tradition': 0.87, 'Universalism: concern': 0.86, 'Universalism: nature': 0.93, 'Universalism: objectivity': 0.76, 'Universalism: tolerance': 0.8, 'avg-precision': 0.86}</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>{'Achievement': 0.7, 'Benevolence: caring': 0.56, 'Benevolence: dependability': 0.03, 'Conformity: interpersonal': 0.04, 'Conformity: rules': 0.5, 'Face': 0.05, 'Hedonism': 0.22, 'Humility': 0.09, 'Power: dominance': 0.04, 'Power: resources': 0.68, 'Security: personal': 0.78, 'Security: societal': 0.71, 'Self-direction: action': 0.71, 'Self-direction: thought': 0.83, 'Stimulation': 0.09, 'Tradition': 0.49, 'Universalism: concern': 0.69, 'Universalism: nature': 0.69, 'Universalism: objectivity': 0.29, 'Universalism: tolerance': 0.19, 'avg-recall': 0.42}</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.272100</td>\n",
       "      <td>0.245149</td>\n",
       "      <td>0.908728</td>\n",
       "      <td>{'Achievement': 0.78, 'Benevolence: caring': 0.64, 'Benevolence: dependability': 0.08, 'Conformity: interpersonal': 0.14, 'Conformity: rules': 0.63, 'Face': 0.08, 'Hedonism': 0.43, 'Humility': 0.24, 'Power: dominance': 0.02, 'Power: resources': 0.82, 'Security: personal': 0.83, 'Security: societal': 0.81, 'Self-direction: action': 0.79, 'Self-direction: thought': 0.8, 'Stimulation': 0.21, 'Tradition': 0.66, 'Universalism: concern': 0.8, 'Universalism: nature': 0.8, 'Universalism: objectivity': 0.44, 'Universalism: tolerance': 0.37, 'avg-f1-score': 0.52}</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>{'Achievement': 0.89, 'Benevolence: caring': 0.84, 'Benevolence: dependability': 0.93, 'Conformity: interpersonal': 0.87, 'Conformity: rules': 0.79, 'Face': 0.86, 'Hedonism': 0.92, 'Humility': 0.87, 'Power: dominance': 1.0, 'Power: resources': 0.87, 'Security: personal': 0.88, 'Security: societal': 0.92, 'Self-direction: action': 0.88, 'Self-direction: thought': 0.82, 'Stimulation': 0.84, 'Tradition': 0.88, 'Universalism: concern': 0.87, 'Universalism: nature': 0.97, 'Universalism: objectivity': 0.8, 'Universalism: tolerance': 0.83, 'avg-precision': 0.88}</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>{'Achievement': 0.69, 'Benevolence: caring': 0.51, 'Benevolence: dependability': 0.04, 'Conformity: interpersonal': 0.07, 'Conformity: rules': 0.53, 'Face': 0.04, 'Hedonism': 0.28, 'Humility': 0.14, 'Power: dominance': 0.01, 'Power: resources': 0.77, 'Security: personal': 0.79, 'Security: societal': 0.72, 'Self-direction: action': 0.72, 'Self-direction: thought': 0.79, 'Stimulation': 0.12, 'Tradition': 0.52, 'Universalism: concern': 0.74, 'Universalism: nature': 0.68, 'Universalism: objectivity': 0.3, 'Universalism: tolerance': 0.24, 'avg-recall': 0.44}</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.233827</td>\n",
       "      <td>0.912799</td>\n",
       "      <td>{'Achievement': 0.81, 'Benevolence: caring': 0.6, 'Benevolence: dependability': 0.15, 'Conformity: interpersonal': 0.1, 'Conformity: rules': 0.6, 'Face': 0.05, 'Hedonism': 0.38, 'Humility': 0.29, 'Power: dominance': 0.2, 'Power: resources': 0.86, 'Security: personal': 0.86, 'Security: societal': 0.86, 'Self-direction: action': 0.8, 'Self-direction: thought': 0.81, 'Stimulation': 0.17, 'Tradition': 0.66, 'Universalism: concern': 0.79, 'Universalism: nature': 0.81, 'Universalism: objectivity': 0.47, 'Universalism: tolerance': 0.35, 'avg-f1-score': 0.53}</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>{'Achievement': 0.89, 'Benevolence: caring': 0.92, 'Benevolence: dependability': 0.82, 'Conformity: interpersonal': 1.0, 'Conformity: rules': 0.85, 'Face': 0.89, 'Hedonism': 0.93, 'Humility': 0.89, 'Power: dominance': 0.91, 'Power: resources': 0.87, 'Security: personal': 0.88, 'Security: societal': 0.85, 'Self-direction: action': 0.91, 'Self-direction: thought': 0.86, 'Stimulation': 0.86, 'Tradition': 0.93, 'Universalism: concern': 0.91, 'Universalism: nature': 0.95, 'Universalism: objectivity': 0.83, 'Universalism: tolerance': 0.89, 'avg-precision': 0.89}</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>{'Achievement': 0.74, 'Benevolence: caring': 0.44, 'Benevolence: dependability': 0.08, 'Conformity: interpersonal': 0.05, 'Conformity: rules': 0.47, 'Face': 0.03, 'Hedonism': 0.24, 'Humility': 0.17, 'Power: dominance': 0.11, 'Power: resources': 0.84, 'Security: personal': 0.83, 'Security: societal': 0.86, 'Self-direction: action': 0.71, 'Self-direction: thought': 0.75, 'Stimulation': 0.09, 'Tradition': 0.51, 'Universalism: concern': 0.7, 'Universalism: nature': 0.71, 'Universalism: objectivity': 0.32, 'Universalism: tolerance': 0.22, 'avg-recall': 0.44}</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.248300</td>\n",
       "      <td>0.221939</td>\n",
       "      <td>0.920426</td>\n",
       "      <td>{'Achievement': 0.82, 'Benevolence: caring': 0.7, 'Benevolence: dependability': 0.22, 'Conformity: interpersonal': 0.18, 'Conformity: rules': 0.72, 'Face': 0.15, 'Hedonism': 0.54, 'Humility': 0.28, 'Power: dominance': 0.18, 'Power: resources': 0.88, 'Security: personal': 0.87, 'Security: societal': 0.87, 'Self-direction: action': 0.83, 'Self-direction: thought': 0.82, 'Stimulation': 0.28, 'Tradition': 0.74, 'Universalism: concern': 0.8, 'Universalism: nature': 0.84, 'Universalism: objectivity': 0.54, 'Universalism: tolerance': 0.45, 'avg-f1-score': 0.59}</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>{'Achievement': 0.94, 'Benevolence: caring': 0.86, 'Benevolence: dependability': 0.79, 'Conformity: interpersonal': 0.69, 'Conformity: rules': 0.77, 'Face': 0.83, 'Hedonism': 0.93, 'Humility': 0.92, 'Power: dominance': 0.88, 'Power: resources': 0.9, 'Security: personal': 0.88, 'Security: societal': 0.88, 'Self-direction: action': 0.92, 'Self-direction: thought': 0.87, 'Stimulation': 0.88, 'Tradition': 0.9, 'Universalism: concern': 0.93, 'Universalism: nature': 0.95, 'Universalism: objectivity': 0.85, 'Universalism: tolerance': 0.85, 'avg-precision': 0.87}</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>{'Achievement': 0.73, 'Benevolence: caring': 0.59, 'Benevolence: dependability': 0.12, 'Conformity: interpersonal': 0.1, 'Conformity: rules': 0.67, 'Face': 0.08, 'Hedonism': 0.39, 'Humility': 0.16, 'Power: dominance': 0.1, 'Power: resources': 0.86, 'Security: personal': 0.87, 'Security: societal': 0.86, 'Self-direction: action': 0.76, 'Self-direction: thought': 0.78, 'Stimulation': 0.17, 'Tradition': 0.63, 'Universalism: concern': 0.7, 'Universalism: nature': 0.75, 'Universalism: objectivity': 0.4, 'Universalism: tolerance': 0.3, 'avg-recall': 0.5}</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.209145</td>\n",
       "      <td>0.926748</td>\n",
       "      <td>{'Achievement': 0.84, 'Benevolence: caring': 0.74, 'Benevolence: dependability': 0.29, 'Conformity: interpersonal': 0.23, 'Conformity: rules': 0.73, 'Face': 0.16, 'Hedonism': 0.62, 'Humility': 0.32, 'Power: dominance': 0.23, 'Power: resources': 0.88, 'Security: personal': 0.89, 'Security: societal': 0.89, 'Self-direction: action': 0.85, 'Self-direction: thought': 0.84, 'Stimulation': 0.37, 'Tradition': 0.72, 'Universalism: concern': 0.85, 'Universalism: nature': 0.85, 'Universalism: objectivity': 0.59, 'Universalism: tolerance': 0.52, 'avg-f1-score': 0.62}</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>{'Achievement': 0.94, 'Benevolence: caring': 0.86, 'Benevolence: dependability': 0.76, 'Conformity: interpersonal': 0.71, 'Conformity: rules': 0.83, 'Face': 0.72, 'Hedonism': 0.92, 'Humility': 0.89, 'Power: dominance': 0.88, 'Power: resources': 0.91, 'Security: personal': 0.91, 'Security: societal': 0.92, 'Self-direction: action': 0.91, 'Self-direction: thought': 0.87, 'Stimulation': 0.82, 'Tradition': 0.94, 'Universalism: concern': 0.91, 'Universalism: nature': 0.97, 'Universalism: objectivity': 0.88, 'Universalism: tolerance': 0.86, 'avg-precision': 0.87}</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>{'Achievement': 0.76, 'Benevolence: caring': 0.65, 'Benevolence: dependability': 0.18, 'Conformity: interpersonal': 0.14, 'Conformity: rules': 0.64, 'Face': 0.09, 'Hedonism': 0.47, 'Humility': 0.2, 'Power: dominance': 0.13, 'Power: resources': 0.85, 'Security: personal': 0.87, 'Security: societal': 0.86, 'Self-direction: action': 0.8, 'Self-direction: thought': 0.81, 'Stimulation': 0.24, 'Tradition': 0.59, 'Universalism: concern': 0.79, 'Universalism: nature': 0.76, 'Universalism: objectivity': 0.45, 'Universalism: tolerance': 0.37, 'avg-recall': 0.53}</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.229400</td>\n",
       "      <td>0.199099</td>\n",
       "      <td>0.932184</td>\n",
       "      <td>{'Achievement': 0.84, 'Benevolence: caring': 0.76, 'Benevolence: dependability': 0.26, 'Conformity: interpersonal': 0.37, 'Conformity: rules': 0.75, 'Face': 0.21, 'Hedonism': 0.66, 'Humility': 0.41, 'Power: dominance': 0.28, 'Power: resources': 0.89, 'Security: personal': 0.89, 'Security: societal': 0.91, 'Self-direction: action': 0.88, 'Self-direction: thought': 0.85, 'Stimulation': 0.42, 'Tradition': 0.8, 'Universalism: concern': 0.86, 'Universalism: nature': 0.87, 'Universalism: objectivity': 0.61, 'Universalism: tolerance': 0.58, 'avg-f1-score': 0.65}</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>{'Achievement': 0.95, 'Benevolence: caring': 0.9, 'Benevolence: dependability': 0.87, 'Conformity: interpersonal': 0.79, 'Conformity: rules': 0.88, 'Face': 0.7, 'Hedonism': 0.9, 'Humility': 0.87, 'Power: dominance': 0.86, 'Power: resources': 0.94, 'Security: personal': 0.91, 'Security: societal': 0.94, 'Self-direction: action': 0.93, 'Self-direction: thought': 0.87, 'Stimulation': 0.82, 'Tradition': 0.92, 'Universalism: concern': 0.92, 'Universalism: nature': 0.98, 'Universalism: objectivity': 0.9, 'Universalism: tolerance': 0.86, 'avg-precision': 0.89}</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>{'Achievement': 0.76, 'Benevolence: caring': 0.65, 'Benevolence: dependability': 0.16, 'Conformity: interpersonal': 0.24, 'Conformity: rules': 0.65, 'Face': 0.13, 'Hedonism': 0.52, 'Humility': 0.27, 'Power: dominance': 0.17, 'Power: resources': 0.85, 'Security: personal': 0.88, 'Security: societal': 0.88, 'Self-direction: action': 0.84, 'Self-direction: thought': 0.83, 'Stimulation': 0.28, 'Tradition': 0.71, 'Universalism: concern': 0.82, 'Universalism: nature': 0.79, 'Universalism: objectivity': 0.46, 'Universalism: tolerance': 0.43, 'avg-recall': 0.57}</td>\n",
       "      <td>0.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.218800</td>\n",
       "      <td>0.192105</td>\n",
       "      <td>0.934854</td>\n",
       "      <td>{'Achievement': 0.87, 'Benevolence: caring': 0.78, 'Benevolence: dependability': 0.27, 'Conformity: interpersonal': 0.28, 'Conformity: rules': 0.74, 'Face': 0.25, 'Hedonism': 0.72, 'Humility': 0.45, 'Power: dominance': 0.27, 'Power: resources': 0.91, 'Security: personal': 0.91, 'Security: societal': 0.91, 'Self-direction: action': 0.89, 'Self-direction: thought': 0.87, 'Stimulation': 0.6, 'Tradition': 0.79, 'Universalism: concern': 0.86, 'Universalism: nature': 0.89, 'Universalism: objectivity': 0.63, 'Universalism: tolerance': 0.56, 'avg-f1-score': 0.67}</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>{'Achievement': 0.94, 'Benevolence: caring': 0.88, 'Benevolence: dependability': 0.83, 'Conformity: interpersonal': 0.79, 'Conformity: rules': 0.88, 'Face': 0.72, 'Hedonism': 0.86, 'Humility': 0.87, 'Power: dominance': 0.87, 'Power: resources': 0.93, 'Security: personal': 0.93, 'Security: societal': 0.92, 'Self-direction: action': 0.91, 'Self-direction: thought': 0.86, 'Stimulation': 0.8, 'Tradition': 0.93, 'Universalism: concern': 0.94, 'Universalism: nature': 0.98, 'Universalism: objectivity': 0.89, 'Universalism: tolerance': 0.83, 'avg-precision': 0.88}</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>{'Achievement': 0.81, 'Benevolence: caring': 0.7, 'Benevolence: dependability': 0.16, 'Conformity: interpersonal': 0.17, 'Conformity: rules': 0.64, 'Face': 0.15, 'Hedonism': 0.61, 'Humility': 0.3, 'Power: dominance': 0.16, 'Power: resources': 0.89, 'Security: personal': 0.89, 'Security: societal': 0.91, 'Self-direction: action': 0.87, 'Self-direction: thought': 0.88, 'Stimulation': 0.47, 'Tradition': 0.69, 'Universalism: concern': 0.8, 'Universalism: nature': 0.82, 'Universalism: objectivity': 0.49, 'Universalism: tolerance': 0.42, 'avg-recall': 0.59}</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.214500</td>\n",
       "      <td>0.183149</td>\n",
       "      <td>0.940278</td>\n",
       "      <td>{'Achievement': 0.89, 'Benevolence: caring': 0.81, 'Benevolence: dependability': 0.36, 'Conformity: interpersonal': 0.26, 'Conformity: rules': 0.79, 'Face': 0.22, 'Hedonism': 0.65, 'Humility': 0.44, 'Power: dominance': 0.4, 'Power: resources': 0.92, 'Security: personal': 0.91, 'Security: societal': 0.92, 'Self-direction: action': 0.88, 'Self-direction: thought': 0.87, 'Stimulation': 0.47, 'Tradition': 0.82, 'Universalism: concern': 0.88, 'Universalism: nature': 0.89, 'Universalism: objectivity': 0.72, 'Universalism: tolerance': 0.62, 'avg-f1-score': 0.69}</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>{'Achievement': 0.92, 'Benevolence: caring': 0.9, 'Benevolence: dependability': 0.83, 'Conformity: interpersonal': 0.78, 'Conformity: rules': 0.86, 'Face': 0.85, 'Hedonism': 0.89, 'Humility': 0.87, 'Power: dominance': 0.9, 'Power: resources': 0.96, 'Security: personal': 0.94, 'Security: societal': 0.95, 'Self-direction: action': 0.95, 'Self-direction: thought': 0.88, 'Stimulation': 0.81, 'Tradition': 0.93, 'Universalism: concern': 0.94, 'Universalism: nature': 0.98, 'Universalism: objectivity': 0.88, 'Universalism: tolerance': 0.86, 'avg-precision': 0.89}</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>{'Achievement': 0.87, 'Benevolence: caring': 0.73, 'Benevolence: dependability': 0.23, 'Conformity: interpersonal': 0.16, 'Conformity: rules': 0.73, 'Face': 0.13, 'Hedonism': 0.52, 'Humility': 0.3, 'Power: dominance': 0.26, 'Power: resources': 0.88, 'Security: personal': 0.89, 'Security: societal': 0.9, 'Self-direction: action': 0.83, 'Self-direction: thought': 0.86, 'Stimulation': 0.33, 'Tradition': 0.74, 'Universalism: concern': 0.84, 'Universalism: nature': 0.82, 'Universalism: objectivity': 0.6, 'Universalism: tolerance': 0.48, 'avg-recall': 0.6}</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.204900</td>\n",
       "      <td>0.176632</td>\n",
       "      <td>0.941750</td>\n",
       "      <td>{'Achievement': 0.88, 'Benevolence: caring': 0.82, 'Benevolence: dependability': 0.39, 'Conformity: interpersonal': 0.33, 'Conformity: rules': 0.8, 'Face': 0.25, 'Hedonism': 0.67, 'Humility': 0.44, 'Power: dominance': 0.34, 'Power: resources': 0.92, 'Security: personal': 0.92, 'Security: societal': 0.93, 'Self-direction: action': 0.91, 'Self-direction: thought': 0.87, 'Stimulation': 0.48, 'Tradition': 0.83, 'Universalism: concern': 0.89, 'Universalism: nature': 0.91, 'Universalism: objectivity': 0.64, 'Universalism: tolerance': 0.64, 'avg-f1-score': 0.69}</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>{'Achievement': 0.96, 'Benevolence: caring': 0.92, 'Benevolence: dependability': 0.82, 'Conformity: interpersonal': 0.77, 'Conformity: rules': 0.88, 'Face': 0.78, 'Hedonism': 0.9, 'Humility': 0.87, 'Power: dominance': 0.87, 'Power: resources': 0.97, 'Security: personal': 0.94, 'Security: societal': 0.96, 'Self-direction: action': 0.95, 'Self-direction: thought': 0.89, 'Stimulation': 0.82, 'Tradition': 0.92, 'Universalism: concern': 0.94, 'Universalism: nature': 0.98, 'Universalism: objectivity': 0.92, 'Universalism: tolerance': 0.86, 'avg-precision': 0.9}</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>{'Achievement': 0.82, 'Benevolence: caring': 0.74, 'Benevolence: dependability': 0.26, 'Conformity: interpersonal': 0.21, 'Conformity: rules': 0.73, 'Face': 0.15, 'Hedonism': 0.53, 'Humility': 0.3, 'Power: dominance': 0.21, 'Power: resources': 0.88, 'Security: personal': 0.91, 'Security: societal': 0.9, 'Self-direction: action': 0.87, 'Self-direction: thought': 0.85, 'Stimulation': 0.34, 'Tradition': 0.76, 'Universalism: concern': 0.85, 'Universalism: nature': 0.84, 'Universalism: objectivity': 0.49, 'Universalism: tolerance': 0.51, 'avg-recall': 0.61}</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.199900</td>\n",
       "      <td>0.170762</td>\n",
       "      <td>0.944588</td>\n",
       "      <td>{'Achievement': 0.9, 'Benevolence: caring': 0.85, 'Benevolence: dependability': 0.37, 'Conformity: interpersonal': 0.4, 'Conformity: rules': 0.8, 'Face': 0.25, 'Hedonism': 0.69, 'Humility': 0.47, 'Power: dominance': 0.33, 'Power: resources': 0.93, 'Security: personal': 0.93, 'Security: societal': 0.93, 'Self-direction: action': 0.9, 'Self-direction: thought': 0.88, 'Stimulation': 0.47, 'Tradition': 0.84, 'Universalism: concern': 0.9, 'Universalism: nature': 0.91, 'Universalism: objectivity': 0.73, 'Universalism: tolerance': 0.64, 'avg-f1-score': 0.71}</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>{'Achievement': 0.96, 'Benevolence: caring': 0.92, 'Benevolence: dependability': 0.84, 'Conformity: interpersonal': 0.82, 'Conformity: rules': 0.91, 'Face': 0.79, 'Hedonism': 0.89, 'Humility': 0.89, 'Power: dominance': 0.88, 'Power: resources': 0.97, 'Security: personal': 0.93, 'Security: societal': 0.97, 'Self-direction: action': 0.97, 'Self-direction: thought': 0.91, 'Stimulation': 0.8, 'Tradition': 0.93, 'Universalism: concern': 0.94, 'Universalism: nature': 0.98, 'Universalism: objectivity': 0.9, 'Universalism: tolerance': 0.87, 'avg-precision': 0.9}</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>{'Achievement': 0.85, 'Benevolence: caring': 0.79, 'Benevolence: dependability': 0.24, 'Conformity: interpersonal': 0.26, 'Conformity: rules': 0.71, 'Face': 0.15, 'Hedonism': 0.56, 'Humility': 0.32, 'Power: dominance': 0.21, 'Power: resources': 0.9, 'Security: personal': 0.92, 'Security: societal': 0.9, 'Self-direction: action': 0.83, 'Self-direction: thought': 0.84, 'Stimulation': 0.34, 'Tradition': 0.76, 'Universalism: concern': 0.86, 'Universalism: nature': 0.84, 'Universalism: objectivity': 0.61, 'Universalism: tolerance': 0.51, 'avg-recall': 0.62}</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.196500</td>\n",
       "      <td>0.165924</td>\n",
       "      <td>0.946995</td>\n",
       "      <td>{'Achievement': 0.92, 'Benevolence: caring': 0.86, 'Benevolence: dependability': 0.39, 'Conformity: interpersonal': 0.32, 'Conformity: rules': 0.82, 'Face': 0.25, 'Hedonism': 0.69, 'Humility': 0.46, 'Power: dominance': 0.38, 'Power: resources': 0.92, 'Security: personal': 0.93, 'Security: societal': 0.94, 'Self-direction: action': 0.91, 'Self-direction: thought': 0.89, 'Stimulation': 0.5, 'Tradition': 0.83, 'Universalism: concern': 0.92, 'Universalism: nature': 0.92, 'Universalism: objectivity': 0.75, 'Universalism: tolerance': 0.64, 'avg-f1-score': 0.71}</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>{'Achievement': 0.96, 'Benevolence: caring': 0.92, 'Benevolence: dependability': 0.83, 'Conformity: interpersonal': 0.83, 'Conformity: rules': 0.91, 'Face': 0.78, 'Hedonism': 0.91, 'Humility': 0.91, 'Power: dominance': 0.84, 'Power: resources': 0.96, 'Security: personal': 0.96, 'Security: societal': 0.98, 'Self-direction: action': 0.97, 'Self-direction: thought': 0.91, 'Stimulation': 0.81, 'Tradition': 0.94, 'Universalism: concern': 0.93, 'Universalism: nature': 0.98, 'Universalism: objectivity': 0.91, 'Universalism: tolerance': 0.88, 'avg-precision': 0.91}</td>\n",
       "      <td>0.910000</td>\n",
       "      <td>{'Achievement': 0.88, 'Benevolence: caring': 0.81, 'Benevolence: dependability': 0.25, 'Conformity: interpersonal': 0.2, 'Conformity: rules': 0.74, 'Face': 0.15, 'Hedonism': 0.55, 'Humility': 0.31, 'Power: dominance': 0.24, 'Power: resources': 0.89, 'Security: personal': 0.9, 'Security: societal': 0.91, 'Self-direction: action': 0.86, 'Self-direction: thought': 0.86, 'Stimulation': 0.36, 'Tradition': 0.75, 'Universalism: concern': 0.9, 'Universalism: nature': 0.86, 'Universalism: objectivity': 0.63, 'Universalism: tolerance': 0.5, 'avg-recall': 0.63}</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.189500</td>\n",
       "      <td>0.162153</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>{'Achievement': 0.92, 'Benevolence: caring': 0.87, 'Benevolence: dependability': 0.43, 'Conformity: interpersonal': 0.42, 'Conformity: rules': 0.82, 'Face': 0.26, 'Hedonism': 0.74, 'Humility': 0.52, 'Power: dominance': 0.4, 'Power: resources': 0.93, 'Security: personal': 0.93, 'Security: societal': 0.94, 'Self-direction: action': 0.93, 'Self-direction: thought': 0.89, 'Stimulation': 0.59, 'Tradition': 0.84, 'Universalism: concern': 0.91, 'Universalism: nature': 0.93, 'Universalism: objectivity': 0.74, 'Universalism: tolerance': 0.66, 'avg-f1-score': 0.73}</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>{'Achievement': 0.96, 'Benevolence: caring': 0.94, 'Benevolence: dependability': 0.83, 'Conformity: interpersonal': 0.82, 'Conformity: rules': 0.92, 'Face': 0.75, 'Hedonism': 0.85, 'Humility': 0.88, 'Power: dominance': 0.86, 'Power: resources': 0.96, 'Security: personal': 0.94, 'Security: societal': 0.98, 'Self-direction: action': 0.96, 'Self-direction: thought': 0.88, 'Stimulation': 0.75, 'Tradition': 0.94, 'Universalism: concern': 0.96, 'Universalism: nature': 0.98, 'Universalism: objectivity': 0.92, 'Universalism: tolerance': 0.87, 'avg-precision': 0.9}</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>{'Achievement': 0.87, 'Benevolence: caring': 0.81, 'Benevolence: dependability': 0.29, 'Conformity: interpersonal': 0.28, 'Conformity: rules': 0.75, 'Face': 0.16, 'Hedonism': 0.65, 'Humility': 0.36, 'Power: dominance': 0.26, 'Power: resources': 0.9, 'Security: personal': 0.93, 'Security: societal': 0.91, 'Self-direction: action': 0.9, 'Self-direction: thought': 0.9, 'Stimulation': 0.49, 'Tradition': 0.75, 'Universalism: concern': 0.86, 'Universalism: nature': 0.89, 'Universalism: objectivity': 0.62, 'Universalism: tolerance': 0.53, 'avg-recall': 0.66}</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='522' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/522 00:45 < 00:32, 6.63 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-0.99586725 -0.3767782  -1.618691   ... -2.2841845  -1.493223\n",
      "  -1.4191581 ]\n",
      " [-1.445169   -1.2008815  -1.8080845  ... -2.4601068  -1.5646787\n",
      "  -1.9591643 ]\n",
      " [-1.5797155  -0.37761506 -1.758961   ... -1.5491625  -1.4218438\n",
      "  -1.1616701 ]\n",
      " ...\n",
      " [-0.86567724 -0.32090092 -1.2826891  ... -1.7032816  -1.4690279\n",
      "  -1.2585807 ]\n",
      " [-1.4079206  -1.2061942  -1.7336397  ... -2.1175778  -1.4920988\n",
      "  -1.9853396 ]\n",
      " [-1.2371757  -1.20544    -1.6988695  ... -2.503446   -1.5996706\n",
      "  -2.1539276 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-500\n",
      "Configuration saved in model\\checkpoint-500\\config.json\n",
      "Model weights saved in model\\checkpoint-500\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-500\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-0.66032916 -0.5756153  -1.8054469  ... -3.4041777  -2.0733032\n",
      "  -2.001844  ]\n",
      " [-1.9089694  -0.9590259  -1.7900207  ... -2.8897045  -1.6647724\n",
      "  -1.6906267 ]\n",
      " [-2.2018466  -0.73979414 -1.993017   ... -1.6972917  -1.5490193\n",
      "  -1.0285108 ]\n",
      " ...\n",
      " [-1.1492139  -0.34299868 -1.2520925  ... -1.7401323  -1.8486053\n",
      "  -2.0795798 ]\n",
      " [-1.4318492  -1.4806254  -1.9024537  ... -2.5247517  -1.9043677\n",
      "  -2.4670653 ]\n",
      " [-0.471707   -0.9579265  -1.6186177  ... -3.3656616  -1.7424321\n",
      "  -2.3157954 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-1000\n",
      "Configuration saved in model\\checkpoint-1000\\config.json\n",
      "Model weights saved in model\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-1000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-0.6977703  -0.70429784 -1.5928936  ... -3.8277006  -2.0035567\n",
      "  -2.0153627 ]\n",
      " [-2.5178118  -1.227517   -1.7649035  ... -3.2772493  -1.6338599\n",
      "  -1.48266   ]\n",
      " [-2.3660383  -1.0492253  -2.113728   ... -2.2580276  -1.4622614\n",
      "  -0.82376915]\n",
      " ...\n",
      " [-1.6143966  -0.7123451  -0.71345496 ... -1.7768745  -1.6909084\n",
      "  -2.377874  ]\n",
      " [-1.4009256  -1.7149452  -1.884306   ... -3.054669   -1.9003149\n",
      "  -2.6594615 ]\n",
      " [-0.75414306 -1.1172445  -1.6618327  ... -4.0361285  -1.7833062\n",
      "  -2.240342  ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-1500\n",
      "Configuration saved in model\\checkpoint-1500\\config.json\n",
      "Model weights saved in model\\checkpoint-1500\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-1500\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-1500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-1.0839216  -0.43375742 -1.3302441  ... -3.6918364  -2.2460656\n",
      "  -1.991411  ]\n",
      " [-2.8685415  -0.83797485 -1.8638129  ... -4.0049715  -1.6669064\n",
      "  -1.1944611 ]\n",
      " [-2.580461   -1.0639744  -2.4100552  ... -2.6230488  -1.5402813\n",
      "  -0.46484664]\n",
      " ...\n",
      " [-1.8393244  -0.75750875 -0.83563024 ... -2.092222   -1.9142787\n",
      "  -2.4615285 ]\n",
      " [-1.6088884  -1.6345752  -1.9759625  ... -3.246694   -2.0230885\n",
      "  -3.0647001 ]\n",
      " [-0.7756628  -0.5969174  -1.628373   ... -4.291505   -1.9137481\n",
      "  -2.3933187 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-2000\n",
      "Configuration saved in model\\checkpoint-2000\\config.json\n",
      "Model weights saved in model\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-2000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-1.5487218  -0.95781976 -1.526781   ... -3.6475346  -1.8969129\n",
      "  -1.8763027 ]\n",
      " [-2.6814308  -0.85272294 -1.8022249  ... -4.228257   -1.6241359\n",
      "  -1.1018124 ]\n",
      " [-2.495914   -0.8655821  -2.3987641  ... -2.5479164  -1.3273237\n",
      "   0.01429927]\n",
      " ...\n",
      " [-1.8924335  -0.70817614 -0.60448706 ... -1.9176644  -2.0211694\n",
      "  -2.6297736 ]\n",
      " [-1.4522673  -1.8218279  -2.091036   ... -3.4255877  -2.2181296\n",
      "  -3.2272613 ]\n",
      " [-1.0895567  -0.536473   -1.6640244  ... -4.794584   -2.1493554\n",
      "  -2.3100114 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-2500\n",
      "Configuration saved in model\\checkpoint-2500\\config.json\n",
      "Model weights saved in model\\checkpoint-2500\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-2500\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-2500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-1.6977692  -0.79589915 -1.3830302  ... -3.2416897  -1.2795619\n",
      "  -1.5387716 ]\n",
      " [-2.842717   -0.7605578  -1.9621949  ... -4.4893684  -1.2550279\n",
      "  -0.5041175 ]\n",
      " [-2.7625608  -0.90272707 -2.684544   ... -2.8826535  -1.4239016\n",
      "  -0.13252366]\n",
      " ...\n",
      " [-2.1349163  -0.6717501  -0.4777091  ... -1.7253554  -2.1667876\n",
      "  -2.5121646 ]\n",
      " [-1.4219198  -1.7239165  -2.1265688  ... -3.460277   -2.2357998\n",
      "  -3.2490406 ]\n",
      " [-0.9473982  -0.02293292 -1.4686077  ... -4.679798   -2.0230565\n",
      "  -1.9145645 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-3000\n",
      "Configuration saved in model\\checkpoint-3000\\config.json\n",
      "Model weights saved in model\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-3000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-1.9806805  -1.1143949  -1.2828891  ... -3.3883886  -1.2931874\n",
      "  -1.8015914 ]\n",
      " [-3.1914618  -0.9577099  -2.1202579  ... -4.953905   -1.6978264\n",
      "  -0.63838047]\n",
      " [-2.7042813  -1.0249504  -2.7818673  ... -2.9211168  -1.2756789\n",
      "   0.32923773]\n",
      " ...\n",
      " [-2.279488   -1.1795281  -0.2515972  ... -1.9935608  -2.0187356\n",
      "  -2.5823994 ]\n",
      " [-1.7044549  -1.8311856  -2.3681839  ... -3.9757843  -2.6758018\n",
      "  -3.6132236 ]\n",
      " [-1.3857572  -0.58922243 -1.8129779  ... -5.456257   -2.4803355\n",
      "  -2.3162003 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-3500\n",
      "Configuration saved in model\\checkpoint-3500\\config.json\n",
      "Model weights saved in model\\checkpoint-3500\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-3500\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-3500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-1.9886739  -1.572221   -0.9821259  ... -3.0436869  -0.855272\n",
      "  -1.790288  ]\n",
      " [-3.4831095  -1.4393301  -2.1388807  ... -5.1193104  -1.7114687\n",
      "  -0.8708465 ]\n",
      " [-2.918143   -0.9078186  -2.8919573  ... -2.9346182  -1.3521475\n",
      "   0.543015  ]\n",
      " ...\n",
      " [-2.1873317  -1.5155869  -0.19256522 ... -1.9721799  -1.9566251\n",
      "  -2.801208  ]\n",
      " [-1.010669   -1.9166342  -2.3130307  ... -4.2147894  -2.6852694\n",
      "  -3.4150898 ]\n",
      " [-1.30478    -0.95129466 -1.6808566  ... -5.466354   -2.6672883\n",
      "  -2.7679684 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-4000\n",
      "Configuration saved in model\\checkpoint-4000\\config.json\n",
      "Model weights saved in model\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-4000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-2.236542   -1.6039124  -0.5559064  ... -2.240271   -0.13223806\n",
      "  -1.6101947 ]\n",
      " [-4.233346   -1.4267821  -2.215426   ... -4.8685017  -2.2178068\n",
      "  -0.6224235 ]\n",
      " [-3.0822742  -0.28383094 -2.8978274  ... -2.88123    -1.4816251\n",
      "   0.7665662 ]\n",
      " ...\n",
      " [-2.2627187  -1.2080569  -0.27693868 ... -2.4490337  -1.8985548\n",
      "  -3.0421185 ]\n",
      " [-1.3144767  -1.7076752  -2.6903572  ... -4.1724033  -2.989625\n",
      "  -3.693903  ]\n",
      " [-2.0586333  -0.228881   -1.7872002  ... -5.571581   -3.0132074\n",
      "  -2.5003152 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-4500\n",
      "Configuration saved in model\\checkpoint-4500\\config.json\n",
      "Model weights saved in model\\checkpoint-4500\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-4500\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-4500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-2.6074762  -1.7268702  -0.708203   ... -2.3378358  -0.29578638\n",
      "  -1.6315602 ]\n",
      " [-4.471305   -1.4654095  -2.1891398  ... -4.953129   -2.4436402\n",
      "  -0.9574228 ]\n",
      " [-2.9494576   0.04715278 -3.1129234  ... -3.0468898  -1.6420794\n",
      "   0.94735897]\n",
      " ...\n",
      " [-2.598476   -1.3141024  -0.03598842 ... -2.2787118  -2.1540334\n",
      "  -2.9603915 ]\n",
      " [-1.4174495  -2.0969615  -2.5881567  ... -4.2532334  -3.026553\n",
      "  -3.6287942 ]\n",
      " [-2.1650956  -0.16540441 -1.7239252  ... -5.423764   -3.1625683\n",
      "  -2.3213816 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-5000\n",
      "Configuration saved in model\\checkpoint-5000\\config.json\n",
      "Model weights saved in model\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-5000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-2.54215    -1.796254   -0.7937524  ... -2.1930583   0.04676702\n",
      "  -1.3992277 ]\n",
      " [-4.682432   -1.5957909  -2.2861805  ... -4.871291   -2.5733347\n",
      "  -1.0241528 ]\n",
      " [-2.7952437   0.41114786 -3.0713425  ... -2.4475489  -1.2308716\n",
      "   1.6000886 ]\n",
      " ...\n",
      " [-2.562933   -1.3953933   0.25430647 ... -2.2114964  -1.9187787\n",
      "  -2.8946795 ]\n",
      " [-1.1901999  -1.9648995  -2.8789139  ... -4.7342014  -3.050952\n",
      "  -3.408648  ]\n",
      " [-2.069273   -0.30015847 -1.8328893  ... -5.4917517  -3.1090865\n",
      "  -2.2084966 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-5500\n",
      "Configuration saved in model\\checkpoint-5500\\config.json\n",
      "Model weights saved in model\\checkpoint-5500\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-5500\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-5500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-2.3591661  -2.028615   -0.5918034  ... -1.7635392   0.5374176\n",
      "  -1.397761  ]\n",
      " [-4.708477   -1.8920804  -2.446717   ... -5.165656   -2.4674282\n",
      "  -0.8782112 ]\n",
      " [-2.7871296   0.9276389  -3.1191132  ... -2.528279   -1.4725198\n",
      "   1.7102277 ]\n",
      " ...\n",
      " [-2.6187744  -2.0579867   0.30948418 ... -2.0023386  -1.9668936\n",
      "  -2.9107218 ]\n",
      " [-1.15894    -2.2454345  -2.8700614  ... -4.7457685  -3.3580523\n",
      "  -3.8349257 ]\n",
      " [-1.9011471  -0.53018904 -1.9795599  ... -5.7512045  -3.3988771\n",
      "  -2.6550908 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-6000\n",
      "Configuration saved in model\\checkpoint-6000\\config.json\n",
      "Model weights saved in model\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-6000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-2.044266   -2.1199324  -0.36941814 ... -1.8686059   0.75933444\n",
      "  -1.4023674 ]\n",
      " [-4.482907   -1.8629525  -2.4819136  ... -5.3121347  -2.399824\n",
      "  -0.7186478 ]\n",
      " [-3.1336343   0.3690537  -3.1880784  ... -2.7321134  -1.6513419\n",
      "   1.5444409 ]\n",
      " ...\n",
      " [-2.324767   -1.4654801   0.22770926 ... -2.6301308  -1.7593845\n",
      "  -3.054852  ]\n",
      " [-0.7112258  -2.199721   -2.8629887  ... -4.651863   -3.1588717\n",
      "  -3.6907768 ]\n",
      " [-1.4308047  -0.23278177 -1.9709668  ... -5.93983    -3.126945\n",
      "  -2.325636  ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-6500\n",
      "Configuration saved in model\\checkpoint-6500\\config.json\n",
      "Model weights saved in model\\checkpoint-6500\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-6500\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-6500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-2.4653542  -2.3037546  -0.34745735 ... -1.4556726   0.8992058\n",
      "  -1.3127592 ]\n",
      " [-5.0004654  -1.9699733  -2.6788344  ... -5.183042   -2.8775902\n",
      "  -0.55906165]\n",
      " [-3.2491965   0.45774454 -3.2042491  ... -2.5156796  -1.723923\n",
      "   1.6815212 ]\n",
      " ...\n",
      " [-2.4789488  -1.9476295   0.4740619  ... -2.4395251  -1.9296943\n",
      "  -3.0513778 ]\n",
      " [-1.1084387  -2.2357187  -2.9639678  ... -4.8529215  -3.3280544\n",
      "  -3.633456  ]\n",
      " [-2.0657437  -0.32119194 -2.1715086  ... -5.9192643  -3.5798159\n",
      "  -2.1720624 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-7000\n",
      "Configuration saved in model\\checkpoint-7000\\config.json\n",
      "Model weights saved in model\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-7000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-2.1637654  -2.218455   -0.34218985 ... -1.4556853   1.3037605\n",
      "  -1.3443065 ]\n",
      " [-4.8227015  -1.5815163  -2.8509529  ... -5.311803   -2.3720047\n",
      "  -0.17167938]\n",
      " [-3.169148    1.1303178  -3.0504682  ... -2.094296   -1.4540678\n",
      "   1.9951823 ]\n",
      " ...\n",
      " [-2.3177     -1.8685238   0.22590052 ... -2.7692113  -1.8909805\n",
      "  -3.227003  ]\n",
      " [-1.0646086  -2.2215178  -3.3457522  ... -5.141654   -3.478829\n",
      "  -3.7420557 ]\n",
      " [-2.1899815   0.09984158 -2.2174273  ... -5.855851   -3.344359\n",
      "  -2.2289658 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-7500\n",
      "Configuration saved in model\\checkpoint-7500\\config.json\n",
      "Model weights saved in model\\checkpoint-7500\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-7500\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-7500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-2.0749907  -2.245201   -0.23853643 ... -1.4704179   1.3694927\n",
      "  -1.4331305 ]\n",
      " [-5.1037426  -2.0178003  -2.7916796  ... -5.2518873  -2.6307137\n",
      "  -0.5247283 ]\n",
      " [-3.1657393   1.1937041  -3.0973678  ... -2.0744333  -1.5188184\n",
      "   2.009674  ]\n",
      " ...\n",
      " [-2.2748744  -1.9749529   0.5924165  ... -2.6142244  -1.7536116\n",
      "  -3.1415339 ]\n",
      " [-1.0457032  -2.2076497  -3.3528976  ... -4.9843235  -3.3284364\n",
      "  -3.8241525 ]\n",
      " [-2.230671    0.41268677 -2.262735   ... -5.9082265  -3.2420843\n",
      "  -2.062614  ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-8000\n",
      "Configuration saved in model\\checkpoint-8000\\config.json\n",
      "Model weights saved in model\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-8000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:  [[-2.442029   -2.2736504  -0.20349635 ... -1.1790198   1.6591234\n",
      "  -1.2417303 ]\n",
      " [-5.241252   -2.3119593  -2.996625   ... -5.289796   -2.6547022\n",
      "  -0.41122437]\n",
      " [-3.2484      1.2395121  -3.1731482  ... -2.120273   -1.6078513\n",
      "   1.9277811 ]\n",
      " ...\n",
      " [-2.3152776  -2.0733209   0.53355646 ... -2.4923716  -1.7801456\n",
      "  -3.0719802 ]\n",
      " [-1.0812354  -2.4679484  -3.3221736  ... -4.9322977  -3.5209413\n",
      "  -3.8006194 ]\n",
      " [-2.1942132   0.13273333 -2.283884   ... -5.962903   -3.508463\n",
      "  -1.9925174 ]] [[0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to model\\checkpoint-8500\n",
      "Configuration saved in model\\checkpoint-8500\\config.json\n",
      "Model weights saved in model\\checkpoint-8500\\pytorch_model.bin\n",
      "tokenizer config file saved in model\\checkpoint-8500\\tokenizer_config.json\n",
      "Special tokens file saved in model\\checkpoint-8500\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4176\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "bert_model_evaluation = train_bert_model(train_arguments, 'model', human_values['1'])\n",
    "print(bert_model_evaluation['eval_f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a51594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import predict_bert_model\n",
    "\n",
    "preds = predict_bert_model(valid_arguments, os.path.join('model/', 'prajjwal1/bert-small'), human_values['1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af1affb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1044\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_arguments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06fc6b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "714a0444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[  101,  3510,  2003,  1996,  7209,  8426,  2000,  2619,  1010,  2065,\n",
      "          2111,  2215,  2009,  2027,  2323,  2022,  3499,  4103,   102,  2114,\n",
      "           102,  2057,  2323, 10824,  3510,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}, {'input_ids': tensor([[  101,  3171, 17147,  3073,  1037,  2512,  1011,  2510,  2624,  7542,\n",
      "          2000,  2224,  2114,  3032,  1012,   102,  2114,   102,  2057,  2323,\n",
      "          2203,  1996,  2224,  1997,  3171, 17147,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}]\n",
      "[['[CLS] marriage is the ultimate commitment to someone, if people want it they should be allowrd [SEP] against [SEP] we should abandon marriage [SEP]'], ['[CLS] economic sanctions provide a non - military sanction to use against countries. [SEP] against [SEP] we should end the use of economic sanctions [SEP]']]\n"
     ]
    }
   ],
   "source": [
    "from model import tokenize_and_encode, detokenize_and_decode\n",
    "\n",
    "x = tokenize_and_encode(valid_arguments.head(2))\n",
    "print(x)\n",
    "y = detokenize_and_decode(x)\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "639a22baaa801c989cc954e864d538abd49c0bfeee5c51863e756a16beac61da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
