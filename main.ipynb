{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e470f0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline\n"
     ]
    }
   ],
   "source": [
    "print('Baseline')\n",
    "\n",
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d54d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae31998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4176 entries, 3401 to 3031\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Argument ID                 4176 non-null   object\n",
      " 1   Conclusion                  4176 non-null   object\n",
      " 2   Stance                      4176 non-null   object\n",
      " 3   Premise                     4176 non-null   object\n",
      " 4   Achievement                 4176 non-null   int64 \n",
      " 5   Benevolence: caring         4176 non-null   int64 \n",
      " 6   Benevolence: dependability  4176 non-null   int64 \n",
      " 7   Conformity: interpersonal   4176 non-null   int64 \n",
      " 8   Conformity: rules           4176 non-null   int64 \n",
      " 9   Face                        4176 non-null   int64 \n",
      " 10  Hedonism                    4176 non-null   int64 \n",
      " 11  Humility                    4176 non-null   int64 \n",
      " 12  Power: dominance            4176 non-null   int64 \n",
      " 13  Power: resources            4176 non-null   int64 \n",
      " 14  Security: personal          4176 non-null   int64 \n",
      " 15  Security: societal          4176 non-null   int64 \n",
      " 16  Self-direction: action      4176 non-null   int64 \n",
      " 17  Self-direction: thought     4176 non-null   int64 \n",
      " 18  Stimulation                 4176 non-null   int64 \n",
      " 19  Tradition                   4176 non-null   int64 \n",
      " 20  Universalism: concern       4176 non-null   int64 \n",
      " 21  Universalism: nature        4176 non-null   int64 \n",
      " 22  Universalism: objectivity   4176 non-null   int64 \n",
      " 23  Universalism: tolerance     4176 non-null   int64 \n",
      "dtypes: int64(20), object(4)\n",
      "memory usage: 815.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1044 entries, 2 to 5210\n",
      "Data columns (total 24 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Argument ID                 1044 non-null   object\n",
      " 1   Conclusion                  1044 non-null   object\n",
      " 2   Stance                      1044 non-null   object\n",
      " 3   Premise                     1044 non-null   object\n",
      " 4   Achievement                 1044 non-null   int64 \n",
      " 5   Benevolence: caring         1044 non-null   int64 \n",
      " 6   Benevolence: dependability  1044 non-null   int64 \n",
      " 7   Conformity: interpersonal   1044 non-null   int64 \n",
      " 8   Conformity: rules           1044 non-null   int64 \n",
      " 9   Face                        1044 non-null   int64 \n",
      " 10  Hedonism                    1044 non-null   int64 \n",
      " 11  Humility                    1044 non-null   int64 \n",
      " 12  Power: dominance            1044 non-null   int64 \n",
      " 13  Power: resources            1044 non-null   int64 \n",
      " 14  Security: personal          1044 non-null   int64 \n",
      " 15  Security: societal          1044 non-null   int64 \n",
      " 16  Self-direction: action      1044 non-null   int64 \n",
      " 17  Self-direction: thought     1044 non-null   int64 \n",
      " 18  Stimulation                 1044 non-null   int64 \n",
      " 19  Tradition                   1044 non-null   int64 \n",
      " 20  Universalism: concern       1044 non-null   int64 \n",
      " 21  Universalism: nature        1044 non-null   int64 \n",
      " 22  Universalism: objectivity   1044 non-null   int64 \n",
      " 23  Universalism: tolerance     1044 non-null   int64 \n",
      "dtypes: int64(20), object(4)\n",
      "memory usage: 203.9+ KB\n"
     ]
    }
   ],
   "source": [
    "from preprocess import load_arguments, load_label, load_values_from_json, combine_columns, split_arguments\n",
    "from model import train_bert_model\n",
    "import os\n",
    "\n",
    "# Load arguments\n",
    "df_arguments = load_arguments('data/arguments-training.tsv')\n",
    "\n",
    "# Load json\n",
    "human_values = load_values_from_json('data/value-categories.json')\n",
    "\n",
    "df_labels = load_label(\"data/labels-training.tsv\", human_values[\"1\"])\n",
    "df_full_level = combine_columns(df_arguments, df_labels)\n",
    "train_arguments, valid_arguments = split_arguments(df_full_level)\n",
    "\n",
    "train_arguments.info()\n",
    "valid_arguments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d819a8c",
   "metadata": {},
   "source": [
    "## Random Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ab4ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Prediction\n",
      "Added predictions for 4176 arguments\n",
      "{'accuracy_thresh': 0.27721503376960754,\n",
      " 'f1-score': {'Achievement': 0.403,\n",
      "              'Benevolence: caring': 0.401,\n",
      "              'Benevolence: dependability': 0.254,\n",
      "              'Conformity: interpersonal': 0.082,\n",
      "              'Conformity: rules': 0.356,\n",
      "              'Face': 0.135,\n",
      "              'Hedonism': 0.078,\n",
      "              'Humility': 0.157,\n",
      "              'Power: dominance': 0.16,\n",
      "              'Power: resources': 0.182,\n",
      "              'Security: personal': 0.524,\n",
      "              'Security: societal': 0.447,\n",
      "              'Self-direction: action': 0.408,\n",
      "              'Self-direction: thought': 0.297,\n",
      "              'Stimulation': 0.107,\n",
      "              'Tradition': 0.197,\n",
      "              'Universalism: concern': 0.476,\n",
      "              'Universalism: nature': 0.124,\n",
      "              'Universalism: objectivity': 0.29,\n",
      "              'Universalism: tolerance': 0.227,\n",
      "              'avg-f1-score': 0.265},\n",
      " 'marco-avg-f1score': 0.265}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from baseline import random_prediction\n",
    "from metrics import compute_metrics, print_metrics\n",
    "from utils import extract_true_labels\n",
    "\n",
    "\n",
    "\n",
    "random_prediction_values = random_prediction(human_values['1'], train_arguments)\n",
    "\n",
    "metrics = compute_metrics((random_prediction_values, \n",
    "                           extract_true_labels(train_arguments, human_values['1'])), human_values['1'])\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b8590",
   "metadata": {},
   "source": [
    "## All 1 predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bffc3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline import all_ones\n",
    "\n",
    "one_values = all_ones(human_values['1'], train_arguments)\n",
    "\n",
    "metrics = compute_metrics((one_values, \n",
    "                           extract_true_labels(train_arguments, human_values['1'])), human_values['1'])\n",
    "\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02731a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56813bd9956644329decf4ce82d8c691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccaf3d10377d48ac8a3d13db639c13c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a7c9132eb64702836e9b1ce36e3246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48de6f2a84344e408a973439dd086fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4176 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\akshi\\anaconda3\\envs\\py39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4176\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10440\n",
      "  Number of trainable parameters = 28773908\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='10440' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  314/10440 04:22 < 2:21:58, 1.19 it/s, Epoch 0.60/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_model_evaluation = train_bert_model(train_arguments, 'model', human_values['1'])\n",
    "print(bert_model_evaluation['eval_f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a51594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import predict_bert_model\n",
    "\n",
    "preds = predict_bert_model(valid_arguments, os.path.join('model/', 'prajjwal1/bert-small'), human_values['1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4838af39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1044\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_arguments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a8c6b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb264978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at C:\\Users\\akshi/.cache\\huggingface\\hub\\models--prajjwal1--bert-small\\snapshots\\0ec5f86f27c1a77d704439db5e01c307ea11b9d4\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'input_ids': tensor([[  101,  3510,  2003,  1996,  7209,  8426,  2000,  2619,  1010,  2065,\n",
      "          2111,  2215,  2009,  2027,  2323,  2022,  3499,  4103,   102,  2114,\n",
      "           102,  2057,  2323, 10824,  3510,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1]])}, {'input_ids': tensor([[  101,  3171, 17147,  3073,  1037,  2512,  1011,  2510,  2624,  7542,\n",
      "          2000,  2224,  2114,  3032,  1012,   102,  2114,   102,  2057,  2323,\n",
      "          2203,  1996,  2224,  1997,  3171, 17147,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}]\n",
      "[['[CLS] marriage is the ultimate commitment to someone, if people want it they should be allowrd [SEP] against [SEP] we should abandon marriage [SEP]'], ['[CLS] economic sanctions provide a non - military sanction to use against countries. [SEP] against [SEP] we should end the use of economic sanctions [SEP]']]\n"
     ]
    }
   ],
   "source": [
    "from model import tokenize_and_encode, detokenize_and_decode\n",
    "\n",
    "x = tokenize_and_encode(valid_arguments.head(2))\n",
    "print(x)\n",
    "y = detokenize_and_decode(x)\n",
    "print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "639a22baaa801c989cc954e864d538abd49c0bfeee5c51863e756a16beac61da"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
